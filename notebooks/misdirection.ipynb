{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Misdirection\n",
    "\n",
    "Some experiments into how gpt2-small behaves when generating false output.\n",
    "\n",
    "Semantic structure misdirects the model to produce incorrect output. For example,\n",
    "\n",
    "> They realise John was two years older than Mary. Mary was born before ...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/richardlayte/Work/layterz/experiments/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformer_lens import HookedTransformer\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "# NBVAL_IGNORE_OUTPUT\n",
    "model = HookedTransformer.from_pretrained(\n",
    "    \"gpt2-small\",\n",
    "    center_unembed=True,\n",
    "    center_writing_weights=True,\n",
    "    fold_ln=True,\n",
    "    refactor_factored_attn_matrices=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import permutations, product\n",
    "import einops\n",
    "from fancy_einsum import einsum\n",
    "from rich import print\n",
    "from IPython.display import HTML\n",
    "from circuitsvis.attention import attention_heads\n",
    "from transformer_lens.utils import Slice\n",
    "from plotly import express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = \"png\"\n",
    "\n",
    "def calculate_logit_diff(cache):\n",
    "    pred_tokens = torch.tensor([\n",
    "        [model.to_single_token(c) for c in completions] \n",
    "        for _, completions in cache.prompts\n",
    "    ]).to(device)\n",
    "\n",
    "    resid_directions = cache.model.tokens_to_residual_directions(pred_tokens)\n",
    "    return resid_directions[:, 0] - resid_directions[:, 1]\n",
    "\n",
    "def calculate_head_contribution(cache, towards, layer=-1, pos_slice=-1):\n",
    "    per_head_residual = cache.stack_head_results(\n",
    "        layer=layer, pos_slice=pos_slice,\n",
    "    )\n",
    "\n",
    "    per_head_logit_diffs = einsum(\n",
    "        \"... batch d_model, batch d_model -> ...\",\n",
    "        per_head_residual, towards,\n",
    "    )\n",
    "\n",
    "    return einops.rearrange(\n",
    "        per_head_logit_diffs,\n",
    "        \"(layer head_index) -> layer head_index\",\n",
    "        layer=cache.model.cfg.n_layers,\n",
    "        head_index=cache.model.cfg.n_heads,\n",
    "    )\n",
    "\n",
    "def calculate_attention_patterns(cache, heads, component=\"pattern\"):\n",
    "    patterns = {}\n",
    "    for head in heads:\n",
    "        layer = head // cache.model.cfg.n_heads\n",
    "        label = f'{layer}.{head % cache.model.cfg.n_heads}'\n",
    "        head_index = head % cache.model.cfg.n_heads\n",
    "        patterns[label] = cache[component, layer].mean(dim=0)[head_index]\n",
    "    return patterns\n",
    "\n",
    "def plot_attention_patterns(patterns):\n",
    "    return [\n",
    "        px.imshow(\n",
    "            pattern.cpu(),\n",
    "            color_continuous_scale=\"rdbu\",\n",
    "            color_continuous_midpoint=0.0,\n",
    "            title=label\n",
    "        )\n",
    "        for label, pattern in patterns.items()\n",
    "    ]\n",
    "\n",
    "def plot_attention_pattern(cache, l, h, title=None, cmap=\"rdbu\", example_index=0):\n",
    "    print(f'({l}, {h}) cmap={cmap}, example_index={example_index}')\n",
    "    pattern = cache['attn', l][:, h].mean(dim=0)\n",
    "    prompt = cache.prompts[example_index]\n",
    "    tokens = cache.model.to_str_tokens(prompt[0])\n",
    "    for y in range(len(tokens)):\n",
    "        for x in range(len(tokens)):\n",
    "            if x < y:\n",
    "                pattern[x, y] = -1\n",
    "    fig = px.imshow(\n",
    "        pattern.cpu(),\n",
    "        width=1200, height=800,\n",
    "        color_continuous_scale=cmap,\n",
    "        color_continuous_midpoint=0.0,\n",
    "        title=title,\n",
    "    )\n",
    "    fig.update_coloraxes(showscale=False)\n",
    "    fig.update_layout(\n",
    "        # set background color to white\n",
    "        plot_bgcolor='white',\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        # set background color to white\n",
    "        plot_bgcolor='white',\n",
    "        margin=dict(l=10, r=10, t=20, b=0)\n",
    "    )\n",
    "\n",
    "    for y in range(len(tokens)):\n",
    "        resid_stream = cache.accumulated_resid(l, apply_ln=True)\n",
    "        lnt = cache.model.OV[l, h]\n",
    "        mlp_in = model.W_in[l]\n",
    "        mlp_out = model.W_out[l]\n",
    "        pos_slice = Slice((0, y))\n",
    "\n",
    "        q = cache['q', l].reshape(-1, len(tokens), cache.model.cfg.n_heads * cache.model.cfg.d_head)\n",
    "        q = pos_slice.apply(q, dim=-2)\n",
    "        q_proj = torch.einsum('bik,kj->bij', q, lnt.A)\n",
    "        q_proj = torch.einsum('bik,kl->bil', q_proj, lnt.B)\n",
    "        q_mlped = torch.einsum('bik,kj->bij', q_proj, mlp_in)\n",
    "        q_mlped_out = torch.einsum('bik,kl->bil', q_mlped, mlp_out)\n",
    "        logits = cache.model.unembed(q_mlped_out)\n",
    "        q_preds = logits.argmax(dim=-1)\n",
    "        q_str_preds = cache.model.to_str_tokens(q_preds[0])\n",
    "        \n",
    "        k = cache['k', l].reshape(-1, len(tokens), cache.model.cfg.n_heads * cache.model.cfg.d_head)\n",
    "        k = pos_slice.apply(k, dim=-2)\n",
    "        z = q * k\n",
    "        z_proj = torch.einsum('bik,kj->bij', z, lnt.A)\n",
    "        z_proj = torch.einsum('bik,kl->bil', z_proj, lnt.B)\n",
    "        z_mlped = torch.einsum('bik,kj->bij', z_proj, mlp_in)\n",
    "        z_mlped_out = torch.einsum('bik,kl->bil', z_mlped, mlp_out)\n",
    "        z_logits = cache.model.unembed(z_mlped_out)\n",
    "        z_preds = z_logits.argmax(dim=-1)\n",
    "        z_str_preds = cache.model.to_str_tokens(z_preds[0])\n",
    "\n",
    "        v = cache['v', l].reshape(-1, len(tokens), cache.model.cfg.n_heads * cache.model.cfg.d_head)\n",
    "        v_proj = torch.einsum('bik,kj->bij', v, lnt.A)\n",
    "        v_proj = torch.einsum('bik,kl->bil', v_proj, lnt.B)\n",
    "        v_mlped = torch.einsum('bik,kj->bij', v_proj, mlp_in)\n",
    "        v_mlped_out = torch.einsum('bik,kl->bil', v_mlped, mlp_out)\n",
    "        v_logits = cache.model.unembed(v_mlped_out)\n",
    "        v_preds = v_logits.argmax(dim=-1)\n",
    "        v_str_preds = cache.model.to_str_tokens(v_preds[0])\n",
    "\n",
    "        logits = cache.model.unembed(resid_stream[-1])\n",
    "        pred_tokens = logits.argmax(dim=-1)\n",
    "        pred_str_tokens = cache.model.to_str_tokens(pred_tokens[0])\n",
    "        # use white if current cell is dark, black otherwise\n",
    "        for x in range(y):\n",
    "            text = z_str_preds[x]\n",
    "            if text.startswith(\" \"):\n",
    "                text = text[1:]\n",
    "            if text == '<|endoftext|>':\n",
    "                text = 'EOS'\n",
    "            color = 'black'\n",
    "            if pattern[y, x] < 0:\n",
    "                color = 'white'\n",
    "            fig.add_annotation(\n",
    "                x=x, y=y,\n",
    "                text=text,\n",
    "                textangle=-45,\n",
    "                showarrow=False, xshift=-2, yshift=0, font=dict(size=16, color=color),\n",
    "            )\n",
    "        fig.add_annotation(\n",
    "            x=x, y=y,\n",
    "            text=v_str_preds[y],\n",
    "            showarrow=False, xshift=-2, yshift=0,\n",
    "            font=dict(size=16, color='white'),\n",
    "        )\n",
    "    \n",
    "    for x in range(len(tokens)):\n",
    "        text = tokens[x]\n",
    "        pred_text = pred_str_tokens[x]\n",
    "        q_str = 'test'\n",
    "        if text == '<|endoftext|>':\n",
    "            text = 'EOS'\n",
    "        if pred_text == '<|endoftext|>':\n",
    "            pred_text = 'EOS'\n",
    "        if q_str == '<|endoftext|>':\n",
    "            q_str = 'EOS'\n",
    "        #bottom\n",
    "        fig.add_annotation(x=x, y=y + 1, text=text, showarrow=False, xshift=0, yshift=0, font=dict(size=16, color='black'))\n",
    "        #left\n",
    "        fig.add_annotation(x=-1, y=x, text=text, showarrow=False, xshift=0, yshift=0, font=dict(size=16, color='black'))\n",
    "        #right\n",
    "        fig.add_annotation(x=len(tokens), y=x, text=pred_text, showarrow=False, xshift=0, yshift=0, font=dict(size=16, color='black'))\n",
    "        #top\n",
    "        fig.add_annotation(x=x, y=-1, text=q_str, textangle=90, showarrow=False, xshift=0, yshift=20, font=dict(size=16, color='black'))\n",
    "\n",
    "    fig.update_layout(\n",
    "        width=1200, height=1200,\n",
    "        xaxis=dict(showticklabels=False, showgrid=False, zeroline=False),\n",
    "        yaxis=dict(showticklabels=False, showgrid=False, zeroline=False)\n",
    "    )\n",
    "    return fig\n",
    "\n",
    "def visualize_attention_patterns(heads, cache, token_labels=None):\n",
    "    labels, patterns = [], []\n",
    "    for head in heads:\n",
    "        layer = head // cache.model.cfg.n_heads\n",
    "        head_index = head % cache.model.cfg.n_heads\n",
    "        labels.append(f\"L{layer}H{head_index}\")\n",
    "        patterns.append(cache[\"attn\", layer][0, head_index])\n",
    "\n",
    "    patterns = torch.stack(patterns, dim=0)\n",
    "    if token_labels is None:\n",
    "        token_labels = [f\"Token {i}\" for i in range(cache['attn', 0].shape[-1])]\n",
    "\n",
    "    return attention_heads(\n",
    "        attention=patterns, tokens=token_labels, attention_head_names=labels\n",
    "    ).show_code()\n",
    "\n",
    "def generate_prompts(templates, names):\n",
    "    return [\n",
    "        (prompt.format(S, IO), (IO, S))\n",
    "        for prompt, (S, IO) in product(templates, permutations(names, 2))\n",
    "    ]\n",
    "\n",
    "def plot_grid(figs, rows=1, labels=None, title=None, width=1800):\n",
    "    cols = len(figs) // rows + (len(figs) % rows > 0)\n",
    "    labels = labels or [f.layout.title.text for f in figs]\n",
    "    fig = make_subplots(\n",
    "        rows=rows, cols=cols, \n",
    "        subplot_titles=labels,\n",
    "    )\n",
    "\n",
    "    for i, f in enumerate(figs):\n",
    "        fig.add_trace(f.data[0], row=i // cols + 1, col=i % cols + 1)\n",
    "        fig.update_yaxes(row=i // cols + 1, col=i % cols + 1, autorange=\"reversed\", visible=False)\n",
    "        fig.update_xaxes(row=i // cols + 1, col=i % cols + 1, visible=False)\n",
    "\n",
    "    a = figs[0]\n",
    "    layout = a.layout\n",
    "    fig.layout.coloraxis = layout.coloraxis\n",
    "    fig.layout.width = width\n",
    "    fig.layout.height = ((width // cols) + 100) * rows\n",
    "    fig.update_coloraxes(showscale=False)\n",
    "    fig.update_layout(\n",
    "        title=dict(text=title, font=dict(size=32), automargin=False, yref=\"paper\"),\n",
    "    )\n",
    "\n",
    "    return fig\n",
    "\n",
    "def print_max_logits(cache, component='resid_post', layer=-1, k=5):\n",
    "    resid_stream = cache[component, layer]\n",
    "    resid_stream = cache.apply_ln_to_stack(resid_stream, layer)\n",
    "\n",
    "    logits = cache.model.unembed(resid_stream)\n",
    "    logits = cache.apply_ln_to_stack(logits, layer)\n",
    "\n",
    "    top_pred_tokens = torch.topk(logits, k=k, dim=-1).indices.permute(2, 0, 1)\n",
    "    tokens = cache.model.to_tokens([p for p, _ in cache.prompts])\n",
    "    pred_resid_directions = cache.model.tokens_to_residual_directions(top_pred_tokens[:, :, :-1])\n",
    "    token_resid_directions = cache.model.tokens_to_residual_directions(tokens[:, 1:])\n",
    "    data = torch.stack([\n",
    "        (token_resid_directions - token_resid_directions).abs().mean((0, -1)),\n",
    "        *(pred_resid_directions - token_resid_directions).abs().mean((1, -1)),\n",
    "    ])\n",
    "    fig = px.imshow(\n",
    "        data.cpu(),\n",
    "        color_continuous_midpoint=-0.5,\n",
    "        color_continuous_scale=\"rdbu\",\n",
    "    )\n",
    "\n",
    "    example_prompt = cache.model.to_str_tokens(cache.prompts[0][0])[1:] + [\"...\"]\n",
    "    for x in range(len(example_prompt)):\n",
    "        fig.add_annotation(x=x, y=0, text=example_prompt[x], showarrow=False, xshift=0, yshift=0, font=dict(size=16, color='white'))\n",
    "        # use white if current cell is dark, black otherwise\n",
    "        for i in range(k):\n",
    "            color = 'black' if x == len(example_prompt) - 1 else 'white'\n",
    "            fig.add_annotation(\n",
    "                x=x, y=i + 1, \n",
    "                text=cache.model.tokenizer.decode(top_pred_tokens[i, 0, x]), \n",
    "                showarrow=False, xshift=0, yshift=0, font=dict(size=16, color=color),\n",
    "            )\n",
    "\n",
    "    fig.update_layout(\n",
    "        width=1200, height=800,\n",
    "        xaxis=dict(showticklabels=False, showgrid=False, zeroline=False),\n",
    "        yaxis=dict(showticklabels=False, showgrid=False, zeroline=False)\n",
    "    )\n",
    "\n",
    "    fig.update_coloraxes(showscale=False)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create clean and corrupted prompt datasets where the corrupted prompts contradict the initial context. Each set is run against the model so we have access to clean and corrupted cached activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24,\n",
       " 24,\n",
       " (' Mary was two years older than John. Who was born first? Mary was born before',\n",
       "  (' John', ' Mary')),\n",
       " (' Mary was two years older than John. Who was born first? John was born before',\n",
       "  (' John', ' Mary')))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names = (\" Mary\", \" John\", \" Alice\", \" Bob\")\n",
    "prompts = generate_prompts(\n",
    "    [\n",
    "        \"{0} was two years older than{1}. Who was born first?{0} was born before\",\n",
    "        \"{0} was two years younger than{1}. Who was born last?{0} was born after\",\n",
    "    ],\n",
    "    names\n",
    ")\n",
    "\n",
    "corrupted_prompts = generate_prompts(\n",
    "    [\n",
    "        \"{0} was two years older than{1}. Who was born first?{1} was born before\",\n",
    "        \"{0} was two years younger than{1}. Who was born last?{1} was born after\",\n",
    "    ],\n",
    "    names\n",
    ")\n",
    "\n",
    "promtps_equal_length = all([\n",
    "    len(model.to_str_tokens(p)) for p, _ in prompts + corrupted_prompts\n",
    "])\n",
    "\n",
    "if not promtps_equal_length:\n",
    "    print(\"Prompts are not equal length\")\n",
    "len(prompts), len(corrupted_prompts), prompts[0], corrupted_prompts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.remove_all_hook_fns()\n",
    "_, cache = model.run_with_cache([p for p, _ in prompts])\n",
    "cache.prompts = prompts\n",
    "\n",
    "_, corrupted_cache = model.run_with_cache([p for p, _ in corrupted_prompts])\n",
    "corrupted_cache.prompts = corrupted_prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below prints the tokens against the logit value for each. The generated output is added to the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_max_logits(cache, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_max_logits(corrupted_cache, k=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, in the corrupted prompt, we see a slightly increased confidence of the false output (~10% increase).\n",
    "\n",
    "Looking at the attention head patterns many of the top positive and negative contributors are shared, but there are a few noteable exceptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = px.imshow(\n",
    "    calculate_head_contribution(cache, calculate_logit_diff(cache)).cpu(),\n",
    "    labels={\"x\": \"Head\", \"y\": \"Layer\"},\n",
    "    color_continuous_midpoint=0.0,\n",
    "    color_continuous_scale=\"rdbu\",\n",
    ")\n",
    "\n",
    "b = px.imshow(\n",
    "    calculate_head_contribution(corrupted_cache, calculate_logit_diff(corrupted_cache)).cpu(),\n",
    "    labels={\"x\": \"Head\", \"y\": \"Layer\"},\n",
    "    color_continuous_midpoint=0.0,\n",
    "    color_continuous_scale=\"rdbu\",\n",
    ")\n",
    "\n",
    "plot_grid((a, b), labels=[\"Clean\", \"Corrupted\"], title='Head contributions to logit diff', width=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A couple of the top positive heads (e.g. 10.2, 9.8) are shared, but otherwise the top positive contributors for the clean inputs are the top negative contributors for the corrupted inputs (e.g. 9.9, 10.0, 10.6, etc). Many of these are name-mover heads from the IOI paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Postive head patterns\n",
    "\n",
    "We can dig into this further by looking at what those attention patterns attend to for each input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels, patterns = [], []\n",
    "for head in range(144):\n",
    "    layer = head // cache.model.cfg.n_heads\n",
    "    head_index = head % cache.model.cfg.n_heads\n",
    "    labels.append(f\"L{layer}H{head_index}\")\n",
    "    patterns.append(cache[\"attn\", layer][0, head_index])\n",
    "\n",
    "patterns = torch.stack(patterns, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "logit_diff = calculate_logit_diff(cache)\n",
    "per_head_logit_diffs = calculate_head_contribution(cache, logit_diff)\n",
    "heads = torch.topk(per_head_logit_diffs.flatten(), k=k).indices\n",
    "\n",
    "corrupted_logit_diff = calculate_logit_diff(corrupted_cache)\n",
    "corrupted_per_head_logit_diffs = calculate_head_contribution(corrupted_cache, corrupted_logit_diff)\n",
    "corrupted_heads = torch.topk(corrupted_per_head_logit_diffs.flatten(), k=k).indices\n",
    "\n",
    "patterns = calculate_attention_patterns(cache, heads)\n",
    "corrupted_patterns = calculate_attention_patterns(corrupted_cache, corrupted_heads)\n",
    "\n",
    "figs = plot_attention_patterns(patterns)\n",
    "corrupted_figs = plot_attention_patterns(corrupted_patterns)\n",
    "\n",
    "display(plot_grid(figs, title=f'Top {k} attention heads for clean prompts'))\n",
    "plot_grid(corrupted_figs, title=f'Top {k} attention heads for corrupted prompts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_heads = torch.topk(-per_head_logit_diffs.flatten(), k=k).indices\n",
    "neg_corrupted_heads = torch.topk(-corrupted_per_head_logit_diffs.flatten(), k=k).indices\n",
    "\n",
    "neg_patterns = calculate_attention_patterns(cache, neg_heads)\n",
    "neg_corrupted_patterns = calculate_attention_patterns(corrupted_cache, neg_corrupted_heads)\n",
    "\n",
    "neg_figs = plot_attention_patterns(neg_patterns)\n",
    "neg_corrupted_figs = plot_attention_patterns(neg_corrupted_patterns)\n",
    "\n",
    "display(plot_grid(neg_figs, title=f'Bottom {k} attention heads for clean prompts'))\n",
    "plot_grid(neg_corrupted_figs, title=f'Bottom {k} attention heads for corrupted prompts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All heads\n",
    "\n",
    "I find it interesting to plot the full set of heads by layer to get a sense of how they react under different inputs. Sometimes it's easier to see that there is some kind of pattern or common elements and work backwards - instead of only looking at heads that are deemed \"important\" using some metric like logit difference. See the pathological inputs notebook for more on this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heads = calculate_attention_patterns(cache, range(144))\n",
    "corrupted_heads = caheads = calculate_attention_patterns(corrupted_cache, range(144))\n",
    "\n",
    "display(\n",
    "    plot_grid(\n",
    "        plot_attention_patterns(heads),\n",
    "        rows=12,\n",
    "    )\n",
    ")\n",
    "\n",
    "display(\n",
    "    plot_grid(\n",
    "        plot_attention_patterns(corrupted_heads),\n",
    "        rows=12,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation patching\n",
    "\n",
    "We can use activation patching to replace the output of specific components to understand how they contribute to the overall generation process. \n",
    "\n",
    "For example, let's try patching these heads, which demonstrate some interesting differences between the clean and corrupted inputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, cache = model.run_with_cache([p[0] for p in prompts])\n",
    "cache.prompts = prompts\n",
    "_, corrupted_cache = model.run_with_cache([p[0] for p in corrupted_prompts])\n",
    "corrupted_cache.prompts = corrupted_prompts\n",
    "\n",
    "indices = [\n",
    "    (5, 5),\n",
    "    (6, 9),\n",
    "]\n",
    "\n",
    "indices = [ i * cache.model.cfg.n_heads + j for i, j in indices ]\n",
    "heads = calculate_attention_patterns(cache, indices, component='attn')\n",
    "corrupted_heads = caheads = calculate_attention_patterns(corrupted_cache, indices, component='attn')\n",
    "\n",
    "display(\n",
    "    plot_grid(\n",
    "        plot_attention_patterns({\n",
    "            **{f'{k} (clean)': v for k, v in heads.items()},\n",
    "            **{f'{k} (corrupted)': v for k, v in corrupted_heads.items()}\n",
    "        }),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the clean input the heads attend to specific tokens in the first sentence which correspond to the question being asked in the current sentence. In the corrupted inputs, where the model is misdirected to produce a false answer, the heads instead attend to the fullstop marking the end of the first sentence.\n",
    "\n",
    "Swapping the Q activations at these heads between the clean and corrupted caches produces surprising attention patterns. 5.5 is reconstructed almost precisely in both directions, but 6.9 transforms to a very different pattern. However, that pattern is precisely the same in both cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.remove_all_hook_fns()\n",
    "activation = 'attn.hook_q'\n",
    "\n",
    "def swap_attention(index):\n",
    "    def hook_fn(t, hook):\n",
    "        t[:, :, index] = corrupted_cache[f'blocks.{index}.{activation}'][:, :, index]\n",
    "    return hook_fn\n",
    "\n",
    "model.add_hook(f'blocks.5.{activation}', swap_attention(5))\n",
    "model.add_hook(f'blocks.6.{activation}', swap_attention(9))\n",
    "\n",
    "_, ablated_cache = model.run_with_cache([p[0] for p in prompts])\n",
    "ablated_cache.prompts = prompts\n",
    "\n",
    "ablated_heads = calculate_attention_patterns(ablated_cache, indices, component='attn')\n",
    "\n",
    "display(\n",
    "    plot_grid(\n",
    "        plot_attention_patterns({\n",
    "            **{f'{k} (clean)': v for k, v in heads.items()},\n",
    "            **{f'{k} (clean ablated)': v for k, v in ablated_heads.items()},\n",
    "        }),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.remove_all_hook_fns()\n",
    "activation = 'attn.hook_q'\n",
    "\n",
    "def swap_attention(index):\n",
    "    def hook_fn(t, hook):\n",
    "        t[:, :, index] = cache[f'blocks.{index}.{activation}'][:, :, index]\n",
    "    return hook_fn\n",
    "\n",
    "model.add_hook(f'blocks.5.{activation}', swap_attention(5))\n",
    "model.add_hook(f'blocks.6.{activation}', swap_attention(9))\n",
    "\n",
    "_, ablated_cache = model.run_with_cache([p[0] for p in corrupted_prompts])\n",
    "ablated_cache.prompts = corrupted_prompts\n",
    "\n",
    "ablated_heads = calculate_attention_patterns(ablated_cache, indices, component='attn')\n",
    "\n",
    "display(\n",
    "    plot_grid(\n",
    "        plot_attention_patterns({\n",
    "            **{f'{k} (corrupted)': v for k, v in corrupted_heads.items()},\n",
    "            **{f'{k} (corrupted ablated)': v for k, v in ablated_heads.items()},\n",
    "        }),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.remove_all_hook_fns()\n",
    "_, cache = model.run_with_cache([p[0] for p in prompts])\n",
    "cache.prompts = prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([24, 18, 768])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = -1\n",
    "h = -2\n",
    "q = cache['q', l].reshape(-1, 18, 12 * 64)\n",
    "q.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache['k', l].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([24, 18, 768])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = cache['k', l].reshape(-1, 18, 12 * 64)\n",
    "k.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_scores = torch.bmm(q, k.transpose(1, 2))\n",
    "attention_scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaling_factor = torch.sqrt(torch.tensor(768, dtype=torch.float32))\n",
    "scaled_attention_scores = attention_scores / scaling_factor\n",
    "scaled_attention_scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = q + k\n",
    "z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([768, 64]), torch.Size([64, 768]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lnt = cache.model.OV[l, h, :, :]\n",
    "lnt.A.shape, lnt.B.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_proj= torch.einsum('bik,kj->bij', q, lnt.A)\n",
    "q_proj.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_proj_b = torch.einsum('bik,kl->bil', q_proj, lnt.B)\n",
    "q_proj_b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = cache.model.unembed(q_proj_b)\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_tokens = logits.argmax(dim=-1)\n",
    "pred_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([768, 3072]), torch.Size([3072, 768]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_in = model.W_in[l]\n",
    "mlp_out = model.W_out[l]\n",
    "\n",
    "mlp_in.shape, mlp_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([24, 18, 768])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rm = cache[('resid_post', l)]\n",
    "rm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([24, 18, 768])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = cache['v', l].reshape(-1, 18, 12 * 64)\n",
    "v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([24, 4, 768])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformer_lens.utils import Slice\n",
    "pos_slice = Slice((0, 4))\n",
    "vv = pos_slice.apply(v, dim=-2)\n",
    "vv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_slice = slice(0, 11)\n",
    "v = v[:, pos_slice, :]\n",
    "v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "einsum(): the number of subscripts in the equation (3) does not match the number of dimensions (4) for operand 0 and no ellipsis was given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m v \u001b[38;5;241m=\u001b[39m vv\n\u001b[0;32m----> 2\u001b[0m v_proj \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbik,kj->bij\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlnt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mA\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m v_proj \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39meinsum(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbik,kl->bil\u001b[39m\u001b[38;5;124m'\u001b[39m, v_proj, lnt\u001b[38;5;241m.\u001b[39mB)\n",
      "File \u001b[0;32m~/Work/layterz/experiments/venv/lib/python3.10/site-packages/torch/functional.py:380\u001b[0m, in \u001b[0;36meinsum\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    375\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m einsum(equation, \u001b[38;5;241m*\u001b[39m_operands)\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(operands) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_einsum\u001b[38;5;241m.\u001b[39menabled:\n\u001b[1;32m    378\u001b[0m     \u001b[38;5;66;03m# the path for contracting 0 or 1 time(s) is already optimized\u001b[39;00m\n\u001b[1;32m    379\u001b[0m     \u001b[38;5;66;03m# or the user has disabled using opt_einsum\u001b[39;00m\n\u001b[0;32m--> 380\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mequation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperands\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    382\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m opt_einsum\u001b[38;5;241m.\u001b[39mis_available():\n",
      "\u001b[0;31mRuntimeError\u001b[0m: einsum(): the number of subscripts in the equation (3) does not match the number of dimensions (4) for operand 0 and no ellipsis was given"
     ]
    }
   ],
   "source": [
    "v = vv\n",
    "v_proj = torch.einsum('bik,kj->bij', v, lnt.A)\n",
    "v_proj = torch.einsum('bik,kl->bil', v_proj, lnt.B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_mlped = torch.einsum('bik,kj->bij', v_proj, mlp_in)\n",
    "v_mlped_out = torch.einsum('bik,kl->bil', v_mlped, mlp_out)\n",
    "\n",
    "v_mlped.shape, v_mlped_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_logits = cache.model.unembed(v_mlped_out)\n",
    "v_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_preds = v_logits.argmax(dim=-1)\n",
    "v_str_preds = cache.model.to_str_tokens(v_preds[0])\n",
    "v_str_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_str_tokens = cache.model.to_str_tokens(pred_tokens[0])\n",
    "print(pred_str_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_prompts = [p[:100] for p, _ in prompts]\n",
    "_, short_cache = model.run_with_cache(_prompts)\n",
    "corrupted_cache.prompts = _prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interesting = [\n",
    "    (6, 5),\n",
    "    (8, 3),\n",
    "    (10, 8),\n",
    "    (11, 8),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"font-weight: bold\">)</span> <span style=\"color: #808000; text-decoration-color: #808000\">cmap</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>, <span style=\"color: #808000; text-decoration-color: #808000\">example_index</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m(\u001b[0m\u001b[1;36m5\u001b[0m, \u001b[1;36m4\u001b[0m\u001b[1m)\u001b[0m \u001b[33mcmap\u001b[0m=\u001b[3;35mNone\u001b[0m, \u001b[33mexample_index\u001b[0m=\u001b[1;36m11\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "einsum(): subscript k has size 768 for operand 1 which does not broadcast with previously seen size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m\n\u001b[1;32m      3\u001b[0m cmap \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhaline\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 4\u001b[0m \u001b[43mplot_attention_pattern\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandint\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m11\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandint\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m11\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcmap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexample_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandint\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m24\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 97\u001b[0m, in \u001b[0;36mplot_attention_pattern\u001b[0;34m(cache, l, h, title, cmap, example_index)\u001b[0m\n\u001b[1;32m     95\u001b[0m q \u001b[38;5;241m=\u001b[39m cache[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mq\u001b[39m\u001b[38;5;124m'\u001b[39m, l]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(tokens), cache\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mn_heads \u001b[38;5;241m*\u001b[39m cache\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39md_head)\n\u001b[1;32m     96\u001b[0m q \u001b[38;5;241m=\u001b[39m pos_slice\u001b[38;5;241m.\u001b[39mapply(q, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m---> 97\u001b[0m q_proj \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbik,kj->bij\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlnt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mA\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m q_proj \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39meinsum(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbik,kl->bil\u001b[39m\u001b[38;5;124m'\u001b[39m, q_proj, lnt\u001b[38;5;241m.\u001b[39mB)\n\u001b[1;32m     99\u001b[0m q_mlped \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39meinsum(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbik,kj->bij\u001b[39m\u001b[38;5;124m'\u001b[39m, q_proj, mlp_in)\n",
      "File \u001b[0;32m~/Work/layterz/experiments/venv/lib/python3.10/site-packages/torch/functional.py:380\u001b[0m, in \u001b[0;36meinsum\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    375\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m einsum(equation, \u001b[38;5;241m*\u001b[39m_operands)\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(operands) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_einsum\u001b[38;5;241m.\u001b[39menabled:\n\u001b[1;32m    378\u001b[0m     \u001b[38;5;66;03m# the path for contracting 0 or 1 time(s) is already optimized\u001b[39;00m\n\u001b[1;32m    379\u001b[0m     \u001b[38;5;66;03m# or the user has disabled using opt_einsum\u001b[39;00m\n\u001b[0;32m--> 380\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mequation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperands\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    382\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m opt_einsum\u001b[38;5;241m.\u001b[39mis_available():\n",
      "\u001b[0;31mRuntimeError\u001b[0m: einsum(): subscript k has size 768 for operand 1 which does not broadcast with previously seen size 0"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "cmap = 'haline'\n",
    "plot_attention_pattern(\n",
    "    cache,\n",
    "    random.randint(0, 11),\n",
    "    random.randint(0, 11),\n",
    "    cmap=None,\n",
    "    example_index=random.randint(0, 24),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_prompts = [p for p, _ in prompts]\n",
    "_prompts = [model.to_str_tokens(p) for p in _prompts]\n",
    "_prompts = [p[:14] for p in _prompts]\n",
    "_prompts = [''.join(p) for p in _prompts]\n",
    "_prompts = [(p, ()) for p in _prompts]\n",
    "\n",
    "_, short_cache = model.run_with_cache([p for p, _ in _prompts])\n",
    "short_cache.prompts = _prompts\n",
    "\n",
    "_prompts[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_attention_pattern(cache, 8, 3, cmap=None, example_index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_attention_pattern(cache, 8, 3, cmap=None, example_index=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_attention_pattern(cache, 8, 3, cmap=None, example_index=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_attention_pattern(short_cache, 8, 3, cmap=None, example_index=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_attention_pattern(cache, 5, 5, cmap=cmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_attention_pattern(cache, 6, 9, cmap=cmap)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
