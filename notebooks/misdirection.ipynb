{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Misdirection\n",
    "\n",
    "Some experiments into how gpt2-small behaves when generating false output.\n",
    "\n",
    "Semantic structure misdirects the model to produce incorrect output. For example,\n",
    "\n",
    "> They realise John was two years older than Mary. Mary was born before ...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformer_lens import HookedTransformer\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "# NBVAL_IGNORE_OUTPUT\n",
    "model = HookedTransformer.from_pretrained(\n",
    "    \"gpt2-small\",\n",
    "    center_unembed=True,\n",
    "    center_writing_weights=True,\n",
    "    fold_ln=True,\n",
    "    refactor_factored_attn_matrices=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import permutations, product\n",
    "import einops\n",
    "from fancy_einsum import einsum\n",
    "from plotly import express as px\n",
    "from IPython.display import HTML\n",
    "from circuitsvis.attention import attention_heads\n",
    "\n",
    "def calculate_logit_diff(cache):\n",
    "    pred_tokens = torch.tensor([\n",
    "        [model.to_single_token(c) for c in completions] \n",
    "        for _, completions in cache.prompts\n",
    "    ]).to(device)\n",
    "\n",
    "    resid_directions = cache.model.tokens_to_residual_directions(pred_tokens)\n",
    "    return resid_directions[:, 0] - resid_directions[:, 1]\n",
    "\n",
    "def calculate_head_contribution(cache):\n",
    "    per_head_residual, labels = cache.stack_head_results(\n",
    "        layer=-1, pos_slice=-1, return_labels=True\n",
    "    )\n",
    "\n",
    "    logit_diff_directions = calculate_logit_diff(cache)\n",
    "    per_head_logit_diffs = einsum(\n",
    "        \"... batch d_model, batch d_model -> ...\",\n",
    "        per_head_residual, logit_diff_directions,\n",
    "    ) / len(cache.prompts)\n",
    "\n",
    "    return einops.rearrange(\n",
    "        per_head_logit_diffs,\n",
    "        \"(layer head_index) -> layer head_index\",\n",
    "        layer=cache.model.cfg.n_layers,\n",
    "        head_index=cache.model.cfg.n_heads,\n",
    "    )\n",
    "\n",
    "def plot_head_contribution(cache, layer=-1, pos_slice=-1):\n",
    "    per_head_residual, labels = cache.stack_head_results(\n",
    "        layer=-1, pos_slice=-1, return_labels=True\n",
    "    )\n",
    "\n",
    "    logit_diff_directions = calculate_logit_diff(cache)\n",
    "    per_head_logit_diffs = einsum(\n",
    "        \"... batch d_model, batch d_model -> ...\",\n",
    "        per_head_residual, logit_diff_directions,\n",
    "    ) / len(cache.prompts)\n",
    "\n",
    "    per_head_logit_diffs = einops.rearrange(\n",
    "        per_head_logit_diffs,\n",
    "        \"(layer head_index) -> layer head_index\",\n",
    "        layer=cache.model.cfg.n_layers,\n",
    "        head_index=cache.model.cfg.n_heads,\n",
    "    )\n",
    "\n",
    "    return px.imshow(\n",
    "        per_head_logit_diffs.cpu(),\n",
    "        labels={\"x\": \"Head\", \"y\": \"Layer\"},\n",
    "        title=\"Logit Difference From Each Head\",\n",
    "        color_continuous_midpoint=0.0,\n",
    "        color_continuous_scale=\"RdBu\",\n",
    "    )\n",
    "\n",
    "def visualize_attention_patterns(heads, cache, prompts):\n",
    "    labels, patterns = [], []\n",
    "    for head in heads:\n",
    "        layer = head // model.cfg.n_heads\n",
    "        head_index = head % model.cfg.n_heads\n",
    "        labels.append(f\"L{layer}H{head_index}\")\n",
    "        patterns.append(cache[\"attn\", layer][0, head_index])\n",
    "\n",
    "    patterns = torch.stack(patterns, dim=0)\n",
    "    tokens = model.to_str_tokens(prompts[0][0])\n",
    "\n",
    "    return attention_heads(\n",
    "        attention=patterns, tokens=tokens, attention_head_names=labels\n",
    "    ).show_code()\n",
    "\n",
    "def generate_prompts(t):\n",
    "    names = (\" Mary\", \" John\", \" Alice\")\n",
    "    places = (\" shops\", \" park\", \" beach\")\n",
    "    things = (\" bag\", \" ball\", \" book\")\n",
    "\n",
    "    templates = [\n",
    "        (\n",
    "            t, (place, thing), permutations(names, 2),\n",
    "        )\n",
    "        for place, thing in product(places, things)\n",
    "    ]\n",
    "\n",
    "    return [\n",
    "        (prompt.format(place, thing, S, IO), (IO, S))\n",
    "        for prompt, (place, thing), names in templates\n",
    "        for S, IO in names\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = generate_prompts(\n",
    "    \"They realised that{2} was two years older than{3}. Who was born earliest?{2} was born before\"\n",
    ")\n",
    "prompts[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, cache = model.run_with_cache([p[0] for p in prompts])\n",
    "cache.prompts = prompts\n",
    "plot_head_contribution(cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in range(model.cfg.n_layers):\n",
    "    resid_stream = cache['resid_post', layer][0]\n",
    "    resid_stream = cache.apply_ln_to_stack(resid_stream, layer)\n",
    "    logits = model.unembed(resid_stream)\n",
    "    pred_token = logits[:, -1, :].argmax(-1)\n",
    "    pred = model.tokenizer.decode(pred_token)\n",
    "    print(f'Layer {layer}: {prompts[0][0]} ...{pred}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resid_stream = cache['resid_post', 11][0]\n",
    "resid_stream = cache.apply_ln_to_stack(resid_stream, 11)\n",
    "logits = model.unembed(resid_stream)\n",
    "pred_token = logits[:, -1, :].argmax(-1)\n",
    "pred = model.tokenizer.decode(pred_token)\n",
    "print(f'Layer {11}: {prompts[0][0]} ...{pred}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_head_logit_diffs = calculate_head_contribution(cache)\n",
    "heads = torch.topk(per_head_logit_diffs.flatten(), k=16).indices\n",
    "HTML(visualize_attention_patterns(heads, cache, prompts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_heads = torch.topk(-per_head_logit_diffs.flatten(), k=16).indices\n",
    "HTML(visualize_attention_patterns(neg_heads, cache, prompts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrupted_prompts = generate_prompts(\n",
    "    \"They realised that{2} was two years older than{3}. Who was born earliest?{3} was born before{2}\"\n",
    ")\n",
    "corrupted_prompts[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, corrupted_cache = model.run_with_cache([p[0] for p in corrupted_prompts])\n",
    "corrupted_cache.prompts = corrupted_prompts\n",
    "\n",
    "plot_head_contribution(corrupted_cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrupted_per_head_logit_diffs = calculate_head_contribution(corrupted_cache)\n",
    "corrupted_heads = torch.topk(corrupted_per_head_logit_diffs.flatten(), k=16).indices\n",
    "HTML(visualize_attention_patterns(corrupted_heads, corrupted_cache, corrupted_prompts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_corrupted_heads = torch.topk(-corrupted_per_head_logit_diffs.flatten(), k=16).indices\n",
    "HTML(visualize_attention_patterns(neg_corrupted_heads, corrupted_cache, corrupted_prompts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = [\n",
    "    (1, 11),\n",
    "    (3, 0),\n",
    "    (5, 5),\n",
    "    (6, 9),\n",
    "]\n",
    "\n",
    "indices = [\n",
    "    (i * model.cfg.n_heads + j)\n",
    "    for i, j in indices\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(visualize_attention_patterns(indices, cache, prompts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(visualize_attention_patterns(indices, corrupted_cache, corrupted_prompts))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
