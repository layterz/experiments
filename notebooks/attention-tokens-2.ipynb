{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention tokens\n",
    "\n",
    "Attention heads appear to learn distinct sets of \"attention tokens\", which act as a compacted vocabulary set used to describe the input within that head. I define dttention tokens as the element-wise (Hadamard) product of each query against each key in the input up to and including the query's position. This is then projected onto the residual stream using the head's OV matrix and unembeded to decode a specific token. This token is the attention token.\n",
    "\n",
    "Using this approach, different heads learn different attention token sets, but there are some common tokens shared between all heads (e.g. \" the\" \"<NEWLINE>\", \" and\"). Further, heads consistently convert inputs that are structurally or semantically similar, but use different language, to similar patterns of attention tokens. The patterns that heads use to represent inputs varies - some massively contract the input sequence into chunks while others appear linear or expand the input. Intuitevely, this tracks with some known behaviours - e.g. 10.7, known to supress copying, expands input, which might be necessary for it's task given the focus on tokens and not structure.\n",
    "\n",
    "At the very least, this feels like a compelling way to categorise attention heads based on how they tend to restructure given inputs. But, I hope that it may also point to a direction for understanding attention mechanisms in a more rigourous way - see the final section for some highly speculative on how some ideas from group theory and physics could be used to help further this goal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformer_lens import HookedTransformer, SVDInterpreter\n",
    "\n",
    "from utils import *\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "# NBVAL_IGNORE_OUTPUT\n",
    "model = HookedTransformer.from_pretrained(\n",
    "    \"gpt2-small\",\n",
    "    center_unembed=True,\n",
    "    center_writing_weights=True,\n",
    "    fold_ln=True,\n",
    "    refactor_factored_attn_matrices=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot below allows you to explore how attention tokens are generated. Note, the colourscale is based on the order in which the attention tokens are discovered during generation meaning brighter colours represent tokens generated later in the process. This means that colours don't represent absolute token values and instead more of a relative position within the generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bd6d90c18c34380b44d3754fc73c0bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value=\"<h2 style='font-size: 14; text-align: center;'>Attention tokens: 0.10</h2>\"), HTML(…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cache = run_prompts(model, \"The next sentence is false. The previous sentence is true.\")\n",
    "\n",
    "l, h = 0, 10\n",
    "plot_grid(\n",
    "    plot_attn(cache, l, h), \n",
    "    title=f\"Attention tokens: {l}.{h}\",\n",
    "    footer=f'<i>{cache.prompts[0]}</i>'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 46, 12, 64]), torch.Size([1, 46, 64]), torch.Size([1, 46, 64]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = cache['q', l][:, :, :, :]\n",
    "k = cache['k', l][:, :, h, :]\n",
    "v = cache['v', l][:, :, h, :]\n",
    "\n",
    "q.shape, k.shape, v.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a sense of how attention tokens are generated I want to test over a reasonably large and varied inputs. For the time being I'm using the imdb dataset because it contains a wide variety of natural language from multiple authors and is relatively small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"imdb\")\n",
    "len(dataset[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b93d6071f6ce44569360cf9cfc79a0d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value=\"<h2 style='font-size: 14; text-align: center;'>\\n    Heads 0.10 and 0.11 for 4 rand…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "inputs = random.choices(dataset[\"train\"][\"text\"], k=4)\n",
    "\n",
    "plots = []\n",
    "l1, h1 = 0, 10\n",
    "for text in inputs:\n",
    "    prompt = ' '.join(text.split(' ')[:16])\n",
    "    cache = run_prompts(model, prompt)\n",
    "    plots.append(plot_attn(cache, l1, h1, hide_labels=True))\n",
    "\n",
    "l2, h2 = 0, 11\n",
    "for text in inputs:\n",
    "    prompt = ' '.join(text.split(' ')[:16])\n",
    "    cache = run_prompts(model, prompt)\n",
    "    plots.append(plot_attn(cache, l2, h2, hide_labels=True))\n",
    "\n",
    "plot_grid(\n",
    "    *plots,\n",
    "    title=f\"Heads {l1}.{h1} and {l2}.{h2} for 4 random IMDB reviews<br>\",\n",
    "    description=f\"\"\"\n",
    "    While specific patterns are hard to pin down, common motifs across the reviews are clearly visible.\n",
    "    \"\"\",\n",
    "    footer='<br>'.join([f'\"<i>{text[:99]}...</i>\"' for text in inputs]),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 46, 46, 6])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [\n",
    "    ('l', 'h', 'at', 'q', 'k', 'v')\n",
    "]\n",
    "\n",
    "data = calculate_attns(cache, l, h)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Tensor' object is not a mapping",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m     cache \u001b[38;5;241m=\u001b[39m run_prompts(model, prompt)\n\u001b[1;32m      8\u001b[0m     attn \u001b[38;5;241m=\u001b[39m calculate_attns(cache, l, h)\n\u001b[0;32m----> 9\u001b[0m     data\u001b[38;5;241m.\u001b[39mappend({\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m'\u001b[39m: prompt,\n\u001b[1;32m     11\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mattn,\n\u001b[1;32m     12\u001b[0m     })\n\u001b[1;32m     14\u001b[0m data[:\u001b[38;5;241m3\u001b[39m]\n",
      "\u001b[0;31mTypeError\u001b[0m: 'Tensor' object is not a mapping"
     ]
    }
   ],
   "source": [
    "inputs = random.choices(dataset[\"train\"][\"text\"], k=4)\n",
    "\n",
    "data = []\n",
    "l, h = 0, 0\n",
    "for text in inputs:\n",
    "    prompt = ' '.join(text.split(' ')[:16])\n",
    "    cache = run_prompts(model, prompt)\n",
    "    attn = calculate_attns(cache, l, h)\n",
    "    data.append({\n",
    "        'prompt': prompt,\n",
    "        **attn,\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention token embedding dimensionality reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph number of attention tokens by input length per head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph number of attention tokens by head index per layer (and reverse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of attention token frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random inputs\n",
    "\n",
    "Testing with random token inputs provides some insights into how the heads behave in a \"default\" state. It also makes it easy to change the input length without worrying about being distracted by how that affects the semantic/grammatical structure of the input.\n",
    "\n",
    "Here are the heads of the first layer using a single random token repeated 62 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12bfe7d0d81941a1a4c177bc2a3b1133",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value=\"<h2 style='font-size: 14; text-align: center;'>Attention tokens</h2>\"), HBox(childr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "token = random.randint(0, 50257)\n",
    "prompt = torch.full((1, 62), token)\n",
    "cache = run_prompts(model, *model.to_string(prompt))\n",
    "plots = plot_attns(cache, range(12), show_grid_labels=False, show_axis=False, show_attn_overlay=False)\n",
    "\n",
    "plot_grid(*plots, title=\"Attention tokens\", description=f'Input: \"{cache.prompts[0][:99]}...\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b9cd67124ae4b56893a33d070e791e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value=\"<h2 style='font-size: 14; text-align: center;'>Attention tokens</h2>\"), HBox(childr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = torch.randint(0, 50257, (1, 62))\n",
    "cache = run_prompts(model, *model.to_string(prompt))\n",
    "plots = plot_attns(cache, range(12), show_grid_labels=False, show_axis=False, show_attn_overlay=False)\n",
    "\n",
    "plot_grid(*plots, title=\"Attention tokens\", description=f'Input: \"{cache.prompts[0][:99]}...\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbf79c1c832341949479c947d7d66d02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value=\"<h2 style='font-size: 14; text-align: center;'>Attention tokens</h2>\"), HBox(childr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = torch.randint(0, 50257, (1, 3)).repeat(1, 21)\n",
    "cache = run_prompts(model, *model.to_string(prompt))\n",
    "plots = plot_attns(cache, range(12), show_grid_labels=False, show_axis=False, show_attn_overlay=False)\n",
    "\n",
    "plot_grid(*plots, title=\"Attention tokens\", description=f'Input: \"{cache.prompts[0][:128]}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's try plotting some more complex repeating sequences using the same head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the same head across random inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A note in input length\n",
    "\n",
    "Input length is important. Up to 31 tokens the attention tokens form surprisingly compact representations of the input across each position. At 32 input tokens some kind of criticality is reached, which dramatically increases the number of attention tokens defined for each input token. You can see this below where the final two lines account for the majority of assigned tokens. It's important to note this doesn't change the existing patterns above, but just makes it much more difficult to view them clearly using this visualization technique.\n",
    "\n",
    "After 32 tokens, things get a bit weird. The attention token structure doesn't remain in the more complex high token count state - instead it appears to alternate between the simple and complex patterns. However, this somewhat depends on the selected random token so it's difficult to highlight a precise sequence.\n",
    "\n",
    "Below is the same head (0.0) plotted using input from 31 up to 48 tokens. You can see the simple and complex pattern evolve twice in this range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c6811177a4d42c1bebdb573a47c593e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value=\"<h2 style='font-size: 14; text-align: center;'>Random inputs between 31 and 48 toke…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "token = random.randint(0, 50257)\n",
    "plots = []\n",
    "for i in range(31, 47, 2):\n",
    "    prompt = torch.full((1, i), token)\n",
    "    cache = run_prompts(model, *model.to_string(prompt))\n",
    "    plots += plot_attns(cache, range(1), show_grid_labels=False, show_axis=False, show_attn_overlay=False)\n",
    "\n",
    "plot_grid(*plots, title=\"Random inputs between 31 and 48 tokens (0.0)\", description=f'Input: \"{cache.prompts[0][:99]}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same behaviour occurs with natural language input, but the threshold is less predictable. The point of criticality appears to align with the start of the most recent \"block\" of text. E.g. if it's a repeating sequence of length 4 the threshold is ~28. This isn't precise, but is relisably closer than 32 as seen with repeated and fully random inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interactive random input plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specific head analysis\n",
    "\n",
    "Random repeating sequences provides the clearest interpretability without being devoid of structure so I'm starting there. Understanding how heads evolve based on changing predictable sequences is likely to provide more general insights, but it's worth noting again the risk of these toy inputs not translating to more realistic language.\n",
    "\n",
    "To provide some structure to the work I plan to evaluate the patterns using this approach:\n",
    "- Take an input sequence (S) and convert it into a sequence of attention tokens (A)\n",
    "- Given the next token (t) generate a new sequence of attention tokens using S + t\n",
    "- Analyse the invariances and symmetries implied by how different sequences produce different attention tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
