{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention tokens\n",
    "\n",
    "Attention heads appear to learn distinct sets of \"attention tokens\", which act as a compacted vocabulary set used to describe the input within that head. I define dttention tokens as the element-wise (Hadamard) product of each query against each key in the input up to and including the query's position. This is then projected onto the residual stream using the head's OV matrix and unembeded to decode a specific token. This token is the attention token.\n",
    "\n",
    "Using this approach, different heads learn different attention token sets, but there are some common tokens shared between all heads (e.g. \" the\" \"<NEWLINE>\", \" and\"). Further, heads consistently convert inputs that are structurally or semantically similar, but use different language, to similar patterns of attention tokens. The patterns that heads use to represent inputs varies - some massively contract the input sequence into chunks while others appear linear or expand the input. Intuitevely, this tracks with some known behaviours - e.g. 10.7, known to supress copying, expands input, which might be necessary for it's task given the focus on tokens and not structure.\n",
    "\n",
    "At the very least, this feels like a compelling way to categorise attention heads based on how they tend to restructure given inputs. But, I hope that it may also point to a direction for understanding attention mechanisms in a more rigourous way - see the final section for some highly speculative on how some ideas from group theory and physics could be used to help further this goal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformer_lens import HookedTransformer, SVDInterpreter\n",
    "\n",
    "from utils import *\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "# NBVAL_IGNORE_OUTPUT\n",
    "model = HookedTransformer.from_pretrained(\n",
    "    \"gpt2-small\",\n",
    "    center_unembed=True,\n",
    "    center_writing_weights=True,\n",
    "    fold_ln=True,\n",
    "    refactor_factored_attn_matrices=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0551446dd2de4784a329236c99201b6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value=\"<h2 style='font-size: 14; text-align: center;'>Attention tokens: 0.10</h2>\"), HBox(…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cache = run_prompts(\n",
    "    model, \n",
    "    \"The next sentence is false. The previous sentence is true.\",\n",
    ")\n",
    "\n",
    "l, h = 0, 10\n",
    "plot_grid(plot_attn(cache, l, h), title=f\"Attention tokens: {l}.{h}\",\n",
    "          description=\"Axis clockwise: query, value, key, input<br>Each attention token is given a unique color starting from 0 and increasing in the order the tokens are discovered - i.e. brighter colors are newer tokens.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random inputs\n",
    "\n",
    "Testing with random token inputs provides some insights into how the heads behave in a \"default\" state. It also makes it easy to change the input length without worrying about being distracted by how that affects the semantic/grammatical structure of the input.\n",
    "\n",
    "Here are the heads of the first layer using a single random token repeated 31 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83fc9ccacdf844069ad5eac088353210",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value=\"<h2 style='font-size: 14; text-align: center;'>Attention tokens</h2>\"), HBox(childr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "token = random.randint(0, 50257)\n",
    "prompt = torch.full((1, 31), token)\n",
    "cache = run_prompts(model, *model.to_string(prompt))\n",
    "plots = plot_attns(cache, range(12), show_grid_labels=False, show_axis=False, show_attn_overlay=False)\n",
    "\n",
    "plot_grid(*plots, title=\"Attention tokens\", description=f'Input: \"{cache.prompts[0][:99]}...\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "31 is important because beyond this length some kind of criticality is reached, which dramatically increases the number of attention tokens defined for each input token. You can see this below where the final two lines account for the majority of assigned tokens. It's important to note this doesn't change the existing patterns above, but just makes it much more difficult to view them clearly using this visualization technique.\n",
    "\n",
    "After 32 tokens, things get a bit weird. The attention token structure doesn't remain in the more complex high token count state - instead it appears to alternate between the simple and complex patterns. However, this somewhat depends on the selected random token so it's difficult to highlight a precise sequence.\n",
    "\n",
    "Below is the same head (0.0) plotted using input from 31 up to 48 tokens. You can see the simple and complex pattern evolve twice in this range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8518ecacf677403ea70cd1c88dde9f81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value=\"<h2 style='font-size: 14; text-align: center;'>Random inputs between 31 and 48 toke…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plots = []\n",
    "for i in range(31, 47, 1):\n",
    "    prompt = torch.full((1, i), token)\n",
    "    cache = run_prompts(model, *model.to_string(prompt))\n",
    "    plots += plot_attns(cache, range(1), show_grid_labels=False, show_axis=False, show_attn_overlay=False)\n",
    "\n",
    "plot_grid(*plots, title=\"Random inputs between 31 and 48 tokens (0.0)\", description=f'Input: \"{cache.prompts[0][:99]}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random sequences\n",
    "\n",
    "Using a sequence of fully random tokens produces similar results, but they are less reliable. For example, patterns will _mostly_ breakdown around 32 tokens, but it often occurs before that point. Patterns, as expected, are not really interpretable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e8ec60aef5248398f4844d81bc5a97e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value=\"<h2 style='font-size: 14; text-align: center;'>Attention tokens</h2>\"), HBox(childr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = torch.randint(0, 50257, (1, 28))\n",
    "cache = run_prompts(model, *model.to_string(prompt))\n",
    "plots = plot_attns(cache, range(12), show_grid_labels=False, show_axis=False, show_attn_overlay=False)\n",
    "\n",
    "plot_grid(*plots, title=\"Attention tokens\", description=f'Input: \"{cache.prompts[0][:99]}...\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random repeating sequences\n",
    "\n",
    "Using repeating random tokens creates more interesting patterns. Although the specific local pattern will vary depending on the selected token, global symmetries clearly emerge and identifiable motifs can be seen for each head. Again, the symmetries are stable and simple up to an input length of 32 and then the same pattern observed above occurs with iterating simple and complex patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85c722519ca541c8a6ee57d472afc470",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value=\"<h2 style='font-size: 14; text-align: center;'>Attention tokens</h2>\"), HBox(childr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = torch.randint(0, 50257, (1, 4)).repeat(1, 7)\n",
    "cache = run_prompts(model, *model.to_string(prompt))\n",
    "plots = plot_attns(cache, range(12), show_grid_labels=False, show_axis=False, show_attn_overlay=False)\n",
    "\n",
    "plot_grid(*plots, title=\"Attention tokens\", description=f'Input: \"{cache.prompts[0][:128]}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real language inputs\n",
    "\n",
    "Indirect object identification (IOI) is a common task used to test models, which follows this structure\n",
    "\n",
    "> John and Mary went to the store. John gave the bag to ...\"\n",
    "\n",
    "The actual task is irrelevant here, but it's useful as an example input because it's more realistic while still being structured and preductable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' Mary and  John went to the store. Mary gave the bag to John.',\n",
       " ' Mary and  Alice went to the store. Mary gave the bag to Alice.',\n",
       " ' Mary and  Bob went to the store. Mary gave the bag to Bob.',\n",
       " ' John and  Mary went to the store. John gave the bag to Mary.',\n",
       " ' John and  Alice went to the store. John gave the bag to Alice.']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_prompts(templates, names):\n",
    "    return [\n",
    "        (prompt.format(S, IO))\n",
    "        for prompt, (S, IO) in product(templates, permutations(names, 2))\n",
    "    ]\n",
    "\n",
    "names = (\" Mary\", \" John\", \" Alice\", \" Bob\")\n",
    "prompts = generate_prompts(\n",
    "    [\n",
    "        \"{0} and {1} went to the store.{0} gave the bag to{1}.\",\n",
    "        \"{0} and {1} went to the zoo.{0} gave a book to{1}.\"\n",
    "    ],\n",
    "    names\n",
    ")\n",
    "\n",
    "prompts[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "cache = run_prompts(model, *prompts[:1])\n",
    "plots = plot_attns(cache, range(12), show_grid_labels=False, show_axis=False, show_attn_overlay=False)\n",
    "\n",
    "plot_grid(*plots, title=\"Attention tokens\", description=f'Input: \"{cache.prompts[0][:128]}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "So far, we've only tested using toy random inputs. Using the same approach on more realistic inputs displays similar, although much harder to interpret, patterns. I'm using the imdb dataset because it offers a reasonably varied colleciton of real language use and is relatively small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"imdb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specific head analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
