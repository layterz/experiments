{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention tokens\n",
    "\n",
    "While exploring some other behaviour I noticed that different heads consistently produce tokens from a compacted subsets of the full vocabulary set when you project the components of a head onto the residual stream without going through the MLP layers. I think this is likely heavily influenced by positional encodings.\n",
    "\n",
    "Each head appears to use it's learn vocabulary subset (the attention tokens) to chunk inputs in ways specific to that head. Some heads compress inputs into a small number attention tokens - as few as 3-4 for 16 token inputs. Others expand and others linearly track or double with input. My intuition is that this represents how heads structurally decompose inputs to represent meaning for their specific tasks and so provides a means for classifying heads.\n",
    "\n",
    "I also look at the macro patterns that emerge across all the heads in the model and find some striking results. For example, the t-SNE plot below shows how the embeddings of the attention tokens are distributed. The plot on the right shows how the attention tokens are shared between heads. Both are quite remarkable - to the degree that I worry this is either an embarassing bug or a trivial artefact of positional encodings.\n",
    "\n",
    "TODO add plot image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformer_lens import HookedTransformer \n",
    "import plotly.io as pio\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils import *\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "# NBVAL_IGNORE_OUTPUT\n",
    "model = HookedTransformer.from_pretrained(\n",
    "    \"gpt2-small\",\n",
    "    center_unembed=True,\n",
    "    center_writing_weights=True,\n",
    "    fold_ln=True,\n",
    "    refactor_factored_attn_matrices=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<contextlib.ExitStack at 0x118451510>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pio.renderers.default = \"png\"\n",
    "plt.ioff()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot below allows you to explore how attention tokens are generated. Note, the colourscale is based on the order in which the attention tokens are discovered during generation meaning brighter colours represent tokens generated later in the process. This means that colours don't represent absolute token values and instead more of a relative position within the generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache = run_prompts(\n",
    "    model, \n",
    "    \"The next sentence is false. The previous sentence is false.\",\n",
    "    \"The next sentence is false. The previous sentence is true.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot, axs = plt.subplots(1, 2, figsize=(16, 5))\n",
    "attn_data = calculate_attns(cache, 0, 1)\n",
    "plot_attn(cache, attn_data, feature_index=0, ax=axs[0])\n",
    "plot_attn(cache, attn_data, feature_index=1, ax=axs[1])\n",
    "\n",
    "figure(plot, title=\"Attention token plot for heads in the first layer\",\n",
    "       footer='The heads in the first layer for the prompt \"The next sentence is false. The previous sentence is true.\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot, axs = plt.subplots(3, 4, figsize=(16, 5))\n",
    "plt.subplots_adjust(wspace=0, hspace=0.5)\n",
    "for i in range(12):\n",
    "    attn_data = calculate_attns(cache, 0, i)\n",
    "    plot_attn(cache, attn_data, ax=axs[i // 4, i % 4], hide_labels=True, title=f\"Head {i}\")\n",
    "\n",
    "figure(plot, title=\"Attention token plot for heads in the first layer\",\n",
    "    description='The heads in the first layer for the prompt \"The next sentence is false. The previous sentence is true.\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a sense of how attention tokens are generated I want to test over a reasonably large and varied inputs. For the time being I'm using the imdb dataset because it contains a wide variety of natural language from multiple authors and is relatively small.\n",
    "\n",
    "The code below extracts 3 random prompts from the dataset and uses them as the input into the model. We then use the cached activations to generate a dataset of attention tokens for each component across each head. The length of each prompt is capped at 16 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"imdb\")\n",
    "inputs = random.choices(dataset[\"train\"][\"text\"], k=3)\n",
    "inputs = model.to_string(model.to_tokens(inputs)[:, :16])\n",
    "cache = run_prompts(model, *inputs)\n",
    "\n",
    "data = generate(cache)\n",
    "df = to_df(data)\n",
    "df.to_csv(\"imdb_example.csv\", index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating the attention token dataset from activations can take a while depending on how many inputs you have and how long they are. To save time, I've pre-generated a dataset of 32 prompts with a token length of 32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load('32x32_attn.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot each of these to get a sense of how the attention tokens are used differently for each head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO add gallery of all 32x32 imdb token plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While it's interesting to look at the patterns that emerge in the token plots, and I think it can be useful for interpreting head behaviour (see below), it doesn't help to understand any macro patterns that may exist across the model heads.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_counts = token_freq_data(model, df, 2, (144, 32, 32, 32))\n",
    "token_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, figsize=(16, 5), )\n",
    "plot_token_frequencies(model, token_counts, colorbar_label='Head', ax=axs[0])\n",
    "plot_unique_tokens_by_head(model, token_counts, s=25, colorbar_label='Layer', ax=axs[1])\n",
    "plot_unique_tokens_by_layer_head(model, token_counts, s=50, colorbar_label='Layer', ax=axs[2])\n",
    "\n",
    "figure(fig, title=\"Token frequencies\", description=\"Token frequencies by layer and head\", footer=\"The next sentence is false. The previous sentence is true.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(16, 5))\n",
    "plot_token_embeddings(model, token_counts, 'PCA', ax=axs[0])\n",
    "plot_token_embeddings(model, token_counts, 'UMAP', ax=axs[1])\n",
    "\n",
    "figure(\n",
    "    fig, \n",
    "    title=\"Token embeddings\", \n",
    "    description=\"Token embeddings by layer and head\", \n",
    "    footer=\"The next sentence is false. The previous sentence is true.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_token_embeddings(model, token_counts, 'TSNE', s=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "at = df[['Input token', 'Input token', 'attn']]\n",
    "at.columns = ['input_token', 'attention_token', 'attention_score']\n",
    "print(at)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def get_head_layer(index, seq_length, batch_size, num_layers, num_heads):\n",
    "    total_tokens = seq_length * batch_size\n",
    "    layer = (index // total_tokens) % num_layers\n",
    "    head = (index // (total_tokens * num_layers)) % num_heads\n",
    "    return layer, head\n",
    "\n",
    "# Assuming at is your DataFrame containing the data\n",
    "seq_length = 32\n",
    "batch_size = 32\n",
    "num_layers = 12\n",
    "num_heads = 12\n",
    "\n",
    "# Add layer and head columns to the DataFrame\n",
    "at['layer'], at['head'] = zip(*at.index.map(lambda x: get_head_layer(x, seq_length, batch_size, num_layers, num_heads)))\n",
    "\n",
    "# Analyze subgroups for each attention head\n",
    "subgroup_data = []\n",
    "for layer in range(num_layers):\n",
    "    for head in range(num_heads):\n",
    "        subgroup_mask = (at['layer'] == layer) & (at['head'] == head)\n",
    "        unique_tokens = at.loc[subgroup_mask, 'attention_token'].unique()\n",
    "        subgroup_data.append({\n",
    "            'Layer': layer,\n",
    "            'Head': head,\n",
    "            'Unique Tokens': len(unique_tokens),\n",
    "            'Tokens': ','.join(map(str, unique_tokens))\n",
    "        })\n",
    "\n",
    "subgroups_df = pd.DataFrame(subgroup_data)\n",
    "\n",
    "# Analyze shared tokens between subgroups\n",
    "shared_token_data = []\n",
    "for i, (layer1, head1) in enumerate(subgroups_df[['Layer', 'Head']].itertuples(index=False)):\n",
    "    for layer2, head2 in subgroups_df[['Layer', 'Head']].iloc[i+1:].itertuples(index=False):\n",
    "        tokens1 = set(map(float, subgroups_df[(subgroups_df['Layer'] == layer1) & (subgroups_df['Head'] == head1)]['Tokens'].iloc[0].split(',')))\n",
    "        tokens2 = set(map(float, subgroups_df[(subgroups_df['Layer'] == layer2) & (subgroups_df['Head'] == head2)]['Tokens'].iloc[0].split(',')))\n",
    "        shared_tokens = tokens1.intersection(tokens2)\n",
    "        if len(shared_tokens) > 0:\n",
    "            shared_token_data.append({\n",
    "                'Subgroup 1': f\"G_({layer1}, {head1})\",\n",
    "                'Subgroup 2': f\"G_({layer2}, {head2})\",\n",
    "                'Shared Tokens': len(shared_tokens),\n",
    "                'Tokens': ','.join(map(str, shared_tokens))\n",
    "            })\n",
    "\n",
    "shared_tokens_df = pd.DataFrame(shared_token_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_tokens_df_sorted = shared_tokens_df.sort_values(by='Shared Tokens', ascending=True)\n",
    "shared_tokens_df_sorted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a pivot table from the shared_tokens_df DataFrame\n",
    "pivot_df = shared_tokens_df.pivot(index='Subgroup 1', columns='Subgroup 2', values='Shared Tokens')\n",
    "\n",
    "# Fill NaN values with 0\n",
    "pivot_df.fillna(0, inplace=True)\n",
    "\n",
    "# Create a heatmap using seaborn\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(pivot_df, annot=False, cmap='inferno', fmt='d', cbar_kws={'label': 'Number of Shared Tokens'})\n",
    "plt.title('Shared Token Overlaps between Subgroups')\n",
    "\n",
    "# Remove the axis tick labels\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random inputs\n",
    "\n",
    "Testing with random token inputs provides some insights into how the heads behave in a \"default\" state. It also makes it easy to change the input length without worrying about being distracted by how that affects the semantic/grammatical structure of the input.\n",
    "\n",
    "Here are the heads of the first layer using a single random token repeated 62 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = random.randint(0, 50257)\n",
    "prompt = torch.full((1, 62), token)\n",
    "cache = run_prompts(model, *model.to_string(prompt))\n",
    "plots = plot_attns(cache, range(12), hide_labels=True)\n",
    "\n",
    "plot_grid(*plots, title=\"Attention tokens\", description=f'Input: \"{cache.prompts[0][:99]}...\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = torch.randint(0, 50257, (1, 62))\n",
    "cache = run_prompts(model, *model.to_string(prompt))\n",
    "plots = plot_attns(cache, range(12), hide_labels=True)\n",
    "\n",
    "plot_grid(*plots, title=\"Attention tokens\", description=f'Input: \"{cache.prompts[0][:99]}...\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = torch.randint(0, 50257, (1, 3)).repeat(1, 21)\n",
    "cache = run_prompts(model, *model.to_string(prompt))\n",
    "plots = plot_attns(cache, range(12), show_grid_labels=False, show_axis=False, show_attn_overlay=False)\n",
    "\n",
    "plot_grid(*plots, title=\"Attention tokens\", description=f'Input: \"{cache.prompts[0][:128]}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A note in input length\n",
    "\n",
    "Input length is important. Up to 31 tokens the attention tokens form surprisingly compact representations of the input across each position. At 32 input tokens some kind of criticality is reached, which dramatically increases the number of attention tokens defined for each input token. You can see this below where the final two lines account for the majority of assigned tokens. It's important to note this doesn't change the existing patterns above, but just makes it much more difficult to view them clearly using this visualization technique.\n",
    "\n",
    "After 32 tokens, things get a bit weird. The attention token structure doesn't remain in the more complex high token count state - instead it appears to alternate between the simple and complex patterns. However, this somewhat depends on the selected random token so it's difficult to highlight a precise sequence.\n",
    "\n",
    "Below is the same head (0.0) plotted using input from 31 up to 48 tokens. You can see the simple and complex pattern evolve twice in this range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = random.randint(0, 50257)\n",
    "plots = []\n",
    "for i in range(31, 47, 2):\n",
    "    prompt = torch.full((1, i), token)\n",
    "    cache = run_prompts(model, *model.to_string(prompt), prepend_bos=False)\n",
    "    plots += plot_attns(cache, range(1), hide_labels=True, prepend_bos=False)\n",
    "\n",
    "plot_grid(*plots, title=\"Random inputs between 31 and 48 tokens (0.0)\", description=f'Input: \"{cache.prompts[0][:99]}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same behaviour occurs with natural language input, but the threshold is less predictable. The point of criticality appears to align with the start of the most recent \"block\" of text. E.g. if it's a repeating sequence of length 4 the threshold is ~28. This isn't precise, but is relisably closer than 32 as seen with repeated and fully random inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots = []\n",
    "tokens = model.to_tokens(random.choice(dataset[\"train\"][\"text\"]))\n",
    "for i in range(31, 47, 2):\n",
    "    prompt = tokens[:, :i]\n",
    "    cache = run_prompts(model, *model.to_string(prompt))\n",
    "    plots += plot_attns(cache, range(1), hide_labels=True)\n",
    "\n",
    "plot_grid(*plots)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specific head analysis\n",
    "\n",
    "Random repeating sequences provides the clearest interpretability without being devoid of structure so I'm starting there. Understanding how heads evolve based on changing predictable sequences is likely to provide more general insights, but it's worth noting again the risk of these toy inputs not translating to more realistic language.\n",
    "\n",
    "To provide some structure to the work I plan to evaluate the patterns using this approach:\n",
    "- Take an input sequence (S) and convert it into a sequence of attention tokens (A)\n",
    "- Given the next token (t) generate a new sequence of attention tokens using S + t\n",
    "- Analyse the invariances and symmetries implied by how different sequences produce different attention tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
