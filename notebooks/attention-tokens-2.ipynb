{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention tokens\n",
    "\n",
    "While exploring some other behaviour I noticed that different heads consistently produce tokens from a compacted subsets of the full vocabulary set when you project the components of a head onto the residual stream without going through the MLP layers. I think this is likely heavily influenced by positional encodings.\n",
    "\n",
    "Each head appears to use it's learn vocabulary subset (the attention tokens) to chunk inputs in ways specific to that head. Some heads compress inputs into a small number attention tokens - as few as 3-4 for 16 token inputs. Others expand and others linearly track or double with input. My intuition is that this represents how heads structurally decompose inputs to represent meaning for their specific tasks and so provides a means for classifying heads.\n",
    "\n",
    "Macro patterns across heads also emerge - particularly when considering the element-wise product of q and k. The plots below show the t-SNE for the embeddings of the element-wise tokens with colors representing layers and the shared element-wise tokens between heads.\n",
    "\n",
    "<div style=\"display: flex; justify-content: center;\">\n",
    "    <img src=\"./imdb-tsne.png\" alt=\"Image 1\" style=\"width: 45%; margin-right: 10px;\">\n",
    "    <img src=\"./shared-token_imdb.png\" alt=\"Image 2\" style=\"width: 45%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformer_lens import HookedTransformer \n",
    "import plotly.io as pio\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils import *\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "# NBVAL_IGNORE_OUTPUT\n",
    "model = HookedTransformer.from_pretrained(\n",
    "    \"gpt2-small\",\n",
    "    center_unembed=True,\n",
    "    center_writing_weights=True,\n",
    "    fold_ln=True,\n",
    "    refactor_factored_attn_matrices=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<contextlib.ExitStack at 0x2b3112f20>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pio.renderers.default = \"png\"\n",
    "plt.ioff()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot below allows you to explore how attention tokens are generated. Note, the colourscale is based on the order in which the attention tokens are discovered during generation meaning brighter colours represent tokens generated later in the process. This means that colours don't represent absolute token values and instead more of a relative position within the generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"The next sentence is false. The previous sentence is true.\"\n",
    "cache = run_prompts(model, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f182171c7be045df96f752e3d2c895ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value=\"<h2 style='font-size: 14; text-align: center;'>Attention tokens for 0.1</h2>\"), HTM…"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l, h = 0, 1\n",
    "attn_data = calculate_attns(cache, l, h)\n",
    "plot = plot_attn(model, attn_data)\n",
    "\n",
    "figure(\n",
    "    plot, \n",
    "    title=f\"Attention tokens for {l}.{h}\", \n",
    "    description=\"\"\"\n",
    "    Each cell represents the element-wise product of the Q and K components of the attention head. Colors increase in brightness based on the order in which the attention tokens were discovered in the raw input.\n",
    "    Axis clockwise from top: query, key, value, and input tokens.\n",
    "    Attention scores for the final position are overlaid as white borders.\n",
    "    \"\"\",\n",
    "    footer=f'Input: {prompt}',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "233f86d5c7884df4a95b7c490922dfb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value=\"<h2 style='font-size: 14; text-align: center;'>Attention token plots for each head …"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot, axs = plt.subplots(3, 4, figsize=(12, 8))\n",
    "plt.subplots_adjust(wspace=0, hspace=0.5)\n",
    "for i in range(12):\n",
    "    attn_data = calculate_attns(cache, l, i)\n",
    "    plot_attn(cache, attn_data, ax=axs[i // 4, i % 4], hide_labels=True, title=f\"Head {i}\")\n",
    "\n",
    "figure(\n",
    "    plot, \n",
    "    title=f\"Attention token plots for each head in layer {l}\",\n",
    "    description=\"\"\"\n",
    "    Note that the colors are relative and don't represent the same atttention token across heads. \n",
    "    Instead the patterns show how each head uses the tokens within its vocabulary set.\n",
    "    \"\"\",\n",
    "    footer=f'Input: {prompt}',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a sense of how attention tokens are generated I want to test over a reasonably large and varied inputs. For the time being I'm using the imdb dataset because it contains a wide variety of natural language from multiple authors and is relatively small.\n",
    "\n",
    "The code below extracts 3 random prompts from the dataset and uses them as the input into the model. We then use the cached activations to generate a dataset of attention tokens for each component across each head. The length of each prompt is capped at 16 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"imdb\")\n",
    "inputs = random.choices(dataset[\"train\"][\"text\"], k=3)\n",
    "inputs = model.to_string(model.to_tokens(inputs)[:, :16])\n",
    "cache = run_prompts(model, *inputs)\n",
    "\n",
    "data = generate(cache)\n",
    "df = to_df(data)\n",
    "df.to_csv(\"imdb_example.csv\", index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating the attention token dataset from activations can take a while depending on how many inputs you have and how long they are. To save time, I've pre-generated a dataset of 32 prompts with a token length of 32 from the imdb dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>layer</th>\n",
       "      <th>head</th>\n",
       "      <th>Input token</th>\n",
       "      <th>attn</th>\n",
       "      <th>hp</th>\n",
       "      <th>q</th>\n",
       "      <th>k</th>\n",
       "      <th>v</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50256.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>357.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>262.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2509051</th>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>379.0</td>\n",
       "      <td>0.002832</td>\n",
       "      <td>47992.0</td>\n",
       "      <td>10473.0</td>\n",
       "      <td>47992.0</td>\n",
       "      <td>2887.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2509052</th>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>379.0</td>\n",
       "      <td>0.002169</td>\n",
       "      <td>47992.0</td>\n",
       "      <td>10473.0</td>\n",
       "      <td>47992.0</td>\n",
       "      <td>2887.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2509053</th>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>379.0</td>\n",
       "      <td>0.004394</td>\n",
       "      <td>47992.0</td>\n",
       "      <td>10473.0</td>\n",
       "      <td>47992.0</td>\n",
       "      <td>2887.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2509054</th>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>379.0</td>\n",
       "      <td>0.016218</td>\n",
       "      <td>23712.0</td>\n",
       "      <td>10473.0</td>\n",
       "      <td>47992.0</td>\n",
       "      <td>2887.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2509055</th>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>379.0</td>\n",
       "      <td>0.020101</td>\n",
       "      <td>23712.0</td>\n",
       "      <td>10473.0</td>\n",
       "      <td>47992.0</td>\n",
       "      <td>2887.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2509056 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         layer  head  Input token      attn       hp        q        k       v\n",
       "0          0.0   0.0      50256.0  1.000000    357.0     11.0     11.0   262.0\n",
       "1         -1.0  -1.0         -1.0 -1.000000     -1.0     -1.0     -1.0    -1.0\n",
       "2         -1.0  -1.0         -1.0 -1.000000     -1.0     -1.0     -1.0    -1.0\n",
       "3         -1.0  -1.0         -1.0 -1.000000     -1.0     -1.0     -1.0    -1.0\n",
       "4         -1.0  -1.0         -1.0 -1.000000     -1.0     -1.0     -1.0    -1.0\n",
       "...        ...   ...          ...       ...      ...      ...      ...     ...\n",
       "2509051   11.0  11.0        379.0  0.002832  47992.0  10473.0  47992.0  2887.0\n",
       "2509052   11.0  11.0        379.0  0.002169  47992.0  10473.0  47992.0  2887.0\n",
       "2509053   11.0  11.0        379.0  0.004394  47992.0  10473.0  47992.0  2887.0\n",
       "2509054   11.0  11.0        379.0  0.016218  23712.0  10473.0  47992.0  2887.0\n",
       "2509055   11.0  11.0        379.0  0.020101  23712.0  10473.0  47992.0  2887.0\n",
       "\n",
       "[2509056 rows x 8 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = load('../data/32x32_attn.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While it's interesting to look at the patterns that emerge in the token plots, and I think it can be useful for interpreting head behaviour (see below), it doesn't help to understand any macro patterns that may exist across the model heads.\n",
    "\n",
    "Calculating fequencies of each token shows a dramatic compaction down to ~1200 tokens for the element-wise product and ~10000 for the Q and K components and ~25000 for V. These numbers seem relatively invariant to actual token input and length - see random token experiments below for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Head</th>\n",
       "      <th>Token</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>Token str</th>\n",
       "      <th>Rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>1325</td>\n",
       "      <td>,</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>461</td>\n",
       "      <td>-</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>198</td>\n",
       "      <td>82</td>\n",
       "      <td>\\n</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>287</td>\n",
       "      <td>94</td>\n",
       "      <td>in</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>290</td>\n",
       "      <td>328</td>\n",
       "      <td>and</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1207</th>\n",
       "      <td>76</td>\n",
       "      <td>9228</td>\n",
       "      <td>223</td>\n",
       "      <td>burgh</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1208</th>\n",
       "      <td>76</td>\n",
       "      <td>22150</td>\n",
       "      <td>292</td>\n",
       "      <td>ultz</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1209</th>\n",
       "      <td>76</td>\n",
       "      <td>23712</td>\n",
       "      <td>3923</td>\n",
       "      <td>eday</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1210</th>\n",
       "      <td>76</td>\n",
       "      <td>30044</td>\n",
       "      <td>407</td>\n",
       "      <td>Jude</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1211</th>\n",
       "      <td>76</td>\n",
       "      <td>47992</td>\n",
       "      <td>4599</td>\n",
       "      <td>cit</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1212 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Head  Token  Frequency Token str  Rank\n",
       "0        0     11       1325         ,   4.0\n",
       "1        0     12        461         -   6.0\n",
       "2        0    198         82        \\n  12.0\n",
       "3        0    287         94        in  11.0\n",
       "4        0    290        328       and   7.0\n",
       "...    ...    ...        ...       ...   ...\n",
       "1207    76   9228        223     burgh   5.0\n",
       "1208    76  22150        292      ultz   4.0\n",
       "1209    76  23712       3923      eday   2.0\n",
       "1210    76  30044        407      Jude   3.0\n",
       "1211    76  47992       4599       cit   1.0\n",
       "\n",
       "[1212 rows x 5 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_counts = token_freq_data(model, df, 4, (144, 32, 32, 32))\n",
    "q_token_counts = token_freq_data(model, df, 5, (144, 32, 32, 32))\n",
    "k_token_counts = token_freq_data(model, df, 6, (144, 32, 32, 32))\n",
    "v_token_counts = token_freq_data(model, df, 7, (144, 32, 32, 32))\n",
    "token_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b75af5926f2847058ea016a4c7d29a6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value=\"<h2 style='font-size: 14; text-align: center;'>Token frequencies</h2>\"), HTML(value…"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(4, 3, figsize=(16, 16))\n",
    "plot_token_frequencies(model, q_token_counts, ax=axs[0][0])\n",
    "plot_unique_tokens_by_head(model, q_token_counts, ax=axs[0][1])\n",
    "plot_unique_tokens_by_layer_head(model, q_token_counts, ax=axs[0][2])\n",
    "plot_token_frequencies(model, k_token_counts, ax=axs[1][0])\n",
    "plot_unique_tokens_by_head(model, k_token_counts, ax=axs[1][1])\n",
    "plot_unique_tokens_by_layer_head(model, k_token_counts, ax=axs[1][2])\n",
    "plot_token_frequencies(model, v_token_counts, ax=axs[2][0])\n",
    "plot_unique_tokens_by_head(model, v_token_counts, ax=axs[2][1])\n",
    "plot_unique_tokens_by_layer_head(model, v_token_counts, ax=axs[2][2])\n",
    "plot_token_frequencies(model, token_counts, ax=axs[3][0])\n",
    "plot_unique_tokens_by_head(model, token_counts, ax=axs[3][1])\n",
    "plot_unique_tokens_by_layer_head(model, token_counts, ax=axs[3][2])\n",
    "figure(\n",
    "    fig,\n",
    "    title=\"Token frequencies\",\n",
    "    description=\"\"\"\n",
    "    From left to right: token frequencies, attention token count by head, and attention token count by layer.\n",
    "    From top to bottom: query, key, value, and element-wise attention tokens.\n",
    "    Colors represent layer index.\n",
    "    \"\"\",\n",
    "    footer=\"IMDB dataset\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've tried various dimensionality reduction techniques, but t-SNE offers by far the most striking visualizations. The plots are unusualy for t-SNE and suggest a highly stucutured dataset, but this is generated from randomly selected snippets natural language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1068925d865476fa5186ece6d48e8cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value=\"<h2 style='font-size: 14; text-align: center;'>t-SNE for attention token embeddings…"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(2, 2, figsize=(12, 8))\n",
    "plot_token_embeddings(model, token_counts, 'TSNE', ax=axs[0][0])\n",
    "plot_token_embeddings(model, q_token_counts, 'TSNE', ax=axs[0][1])\n",
    "plot_token_embeddings(model, k_token_counts, 'TSNE', ax=axs[1][0])\n",
    "plot_token_embeddings(model, v_token_counts, 'TSNE', ax=axs[1][1])\n",
    "\n",
    "figure(\n",
    "    fig,\n",
    "    title=\"t-SNE for attention token embeddings\",\n",
    "    description=\"\"\"\n",
    "    t-SNE representation of the embeddings of the element-wise, query, key, and value attention tokens from top clockwise.\n",
    "    Colours represent the layer index.\n",
    "    \"\"\",\n",
    "    footer=\"IMDB dataset\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating token frequencies shows that patterns exist in how attention tokens are shared between heads - at least one token is shared between every head pair and at most 4 are shared. This forms a pattern that could point to how heads collaborate under different tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subgroup 1</th>\n",
       "      <th>Subgroup 2</th>\n",
       "      <th>Shared Tokens</th>\n",
       "      <th>Tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7928</th>\n",
       "      <td></td>\n",
       "      <td>²</td>\n",
       "      <td>4</td>\n",
       "      <td>290.0,11.0,198.0,262.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>469</th>\n",
       "      <td>D</td>\n",
       "      <td></td>\n",
       "      <td>4</td>\n",
       "      <td>257.0,290.0,11.0,262.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2555</th>\n",
       "      <td>V</td>\n",
       "      <td></td>\n",
       "      <td>4</td>\n",
       "      <td>649.0,290.0,11.0,262.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7312</th>\n",
       "      <td></td>\n",
       "      <td>²</td>\n",
       "      <td>4</td>\n",
       "      <td>290.0,11.0,198.0,262.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3027</th>\n",
       "      <td>Z</td>\n",
       "      <td></td>\n",
       "      <td>4</td>\n",
       "      <td>15961.0,257.0,11.0,262.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7364</th>\n",
       "      <td></td>\n",
       "      <td>©</td>\n",
       "      <td>1</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7365</th>\n",
       "      <td></td>\n",
       "      <td>ª</td>\n",
       "      <td>1</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7366</th>\n",
       "      <td></td>\n",
       "      <td>«</td>\n",
       "      <td>1</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7368</th>\n",
       "      <td></td>\n",
       "      <td>­</td>\n",
       "      <td>1</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4353</th>\n",
       "      <td>i</td>\n",
       "      <td>¹</td>\n",
       "      <td>1</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9234 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Subgroup 1 Subgroup 2  Shared Tokens                    Tokens\n",
       "7928                    ²              4    290.0,11.0,198.0,262.0\n",
       "469           D                        4    257.0,290.0,11.0,262.0\n",
       "2555          V                        4    649.0,290.0,11.0,262.0\n",
       "7312                    ²              4    290.0,11.0,198.0,262.0\n",
       "3027          Z                        4  15961.0,257.0,11.0,262.0\n",
       "...         ...        ...            ...                       ...\n",
       "7364                    ©              1                      11.0\n",
       "7365                    ª              1                      11.0\n",
       "7366                    «              1                      11.0\n",
       "7368                    ­              1                      11.0\n",
       "4353          i          ¹              1                      11.0\n",
       "\n",
       "[9234 rows x 4 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hp = shared_tokens(df)\n",
    "hp.sort_values('Shared Tokens', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/richardlayte/Work/layterz/experiments/venv/lib/python3.10/site-packages/seaborn/utils.py:61: UserWarning:\n",
      "\n",
      "Glyph 128 (\\x80) missing from current font.\n",
      "\n",
      "/Users/richardlayte/Work/layterz/experiments/venv/lib/python3.10/site-packages/seaborn/utils.py:61: UserWarning:\n",
      "\n",
      "Glyph 134 (\\x86) missing from current font.\n",
      "\n",
      "/Users/richardlayte/Work/layterz/experiments/venv/lib/python3.10/site-packages/seaborn/utils.py:61: UserWarning:\n",
      "\n",
      "Glyph 140 (\\x8c) missing from current font.\n",
      "\n",
      "/Users/richardlayte/Work/layterz/experiments/venv/lib/python3.10/site-packages/seaborn/utils.py:61: UserWarning:\n",
      "\n",
      "Glyph 146 (\\x92) missing from current font.\n",
      "\n",
      "/Users/richardlayte/Work/layterz/experiments/venv/lib/python3.10/site-packages/seaborn/utils.py:61: UserWarning:\n",
      "\n",
      "Glyph 152 (\\x98) missing from current font.\n",
      "\n",
      "/Users/richardlayte/Work/layterz/experiments/venv/lib/python3.10/site-packages/seaborn/utils.py:61: UserWarning:\n",
      "\n",
      "Glyph 158 (\\x9e) missing from current font.\n",
      "\n",
      "/Users/richardlayte/Work/layterz/experiments/venv/lib/python3.10/site-packages/seaborn/utils.py:61: UserWarning:\n",
      "\n",
      "Glyph 130 (\\x82) missing from current font.\n",
      "\n",
      "/Users/richardlayte/Work/layterz/experiments/venv/lib/python3.10/site-packages/seaborn/utils.py:61: UserWarning:\n",
      "\n",
      "Glyph 136 (\\x88) missing from current font.\n",
      "\n",
      "/Users/richardlayte/Work/layterz/experiments/venv/lib/python3.10/site-packages/seaborn/utils.py:61: UserWarning:\n",
      "\n",
      "Glyph 142 (\\x8e) missing from current font.\n",
      "\n",
      "/Users/richardlayte/Work/layterz/experiments/venv/lib/python3.10/site-packages/seaborn/utils.py:61: UserWarning:\n",
      "\n",
      "Glyph 148 (\\x94) missing from current font.\n",
      "\n",
      "/Users/richardlayte/Work/layterz/experiments/venv/lib/python3.10/site-packages/seaborn/utils.py:61: UserWarning:\n",
      "\n",
      "Glyph 154 (\\x9a) missing from current font.\n",
      "\n",
      "/Users/richardlayte/Work/layterz/experiments/venv/lib/python3.10/site-packages/seaborn/utils.py:61: UserWarning:\n",
      "\n",
      "Glyph 135 (\\x87) missing from current font.\n",
      "\n",
      "/Users/richardlayte/Work/layterz/experiments/venv/lib/python3.10/site-packages/seaborn/utils.py:61: UserWarning:\n",
      "\n",
      "Glyph 145 (\\x91) missing from current font.\n",
      "\n",
      "/Users/richardlayte/Work/layterz/experiments/venv/lib/python3.10/site-packages/seaborn/utils.py:61: UserWarning:\n",
      "\n",
      "Glyph 150 (\\x96) missing from current font.\n",
      "\n",
      "/Users/richardlayte/Work/layterz/experiments/venv/lib/python3.10/site-packages/seaborn/utils.py:61: UserWarning:\n",
      "\n",
      "Glyph 155 (\\x9b) missing from current font.\n",
      "\n",
      "/Users/richardlayte/Work/layterz/experiments/venv/lib/python3.10/site-packages/seaborn/utils.py:61: UserWarning:\n",
      "\n",
      "Glyph 130 (\\x82) missing from current font.\n",
      "\n",
      "/Users/richardlayte/Work/layterz/experiments/venv/lib/python3.10/site-packages/seaborn/utils.py:61: UserWarning:\n",
      "\n",
      "Glyph 140 (\\x8c) missing from current font.\n",
      "\n",
      "/Users/richardlayte/Work/layterz/experiments/venv/lib/python3.10/site-packages/seaborn/utils.py:61: UserWarning:\n",
      "\n",
      "Glyph 131 (\\x83) missing from current font.\n",
      "\n",
      "/Users/richardlayte/Work/layterz/experiments/venv/lib/python3.10/site-packages/seaborn/utils.py:61: UserWarning:\n",
      "\n",
      "Glyph 141 (\\x8d) missing from current font.\n",
      "\n",
      "/Users/richardlayte/Work/layterz/experiments/venv/lib/python3.10/site-packages/seaborn/utils.py:61: UserWarning:\n",
      "\n",
      "Glyph 151 (\\x97) missing from current font.\n",
      "\n",
      "/Users/richardlayte/Work/layterz/experiments/venv/lib/python3.10/site-packages/seaborn/utils.py:61: UserWarning:\n",
      "\n",
      "Glyph 156 (\\x9c) missing from current font.\n",
      "\n",
      "/Users/richardlayte/Work/layterz/experiments/venv/lib/python3.10/site-packages/seaborn/utils.py:61: UserWarning:\n",
      "\n",
      "Glyph 136 (\\x88) missing from current font.\n",
      "\n",
      "/Users/richardlayte/Work/layterz/experiments/venv/lib/python3.10/site-packages/seaborn/utils.py:61: UserWarning:\n",
      "\n",
      "Glyph 146 (\\x92) missing from current font.\n",
      "\n",
      "/Users/richardlayte/Work/layterz/experiments/venv/lib/python3.10/site-packages/seaborn/utils.py:61: UserWarning:\n",
      "\n",
      "Glyph 130 (\\x82) missing from current font.\n",
      "\n",
      "/Users/richardlayte/Work/layterz/experiments/venv/lib/python3.10/site-packages/seaborn/utils.py:61: UserWarning:\n",
      "\n",
      "Glyph 135 (\\x87) missing from current font.\n",
      "\n",
      "/Users/richardlayte/Work/layterz/experiments/venv/lib/python3.10/site-packages/seaborn/utils.py:61: UserWarning:\n",
      "\n",
      "Glyph 140 (\\x8c) missing from current font.\n",
      "\n",
      "/Users/richardlayte/Work/layterz/experiments/venv/lib/python3.10/site-packages/seaborn/utils.py:61: UserWarning:\n",
      "\n",
      "Glyph 145 (\\x91) missing from current font.\n",
      "\n",
      "/Users/richardlayte/Work/layterz/experiments/venv/lib/python3.10/site-packages/seaborn/utils.py:61: UserWarning:\n",
      "\n",
      "Glyph 150 (\\x96) missing from current font.\n",
      "\n",
      "/Users/richardlayte/Work/layterz/experiments/venv/lib/python3.10/site-packages/seaborn/utils.py:61: UserWarning:\n",
      "\n",
      "Glyph 155 (\\x9b) missing from current font.\n",
      "\n",
      "/Users/richardlayte/Work/layterz/experiments/venv/lib/python3.10/site-packages/seaborn/utils.py:61: UserWarning:\n",
      "\n",
      "Glyph 131 (\\x83) missing from current font.\n",
      "\n",
      "/Users/richardlayte/Work/layterz/experiments/venv/lib/python3.10/site-packages/seaborn/utils.py:61: UserWarning:\n",
      "\n",
      "Glyph 136 (\\x88) missing from current font.\n",
      "\n",
      "/Users/richardlayte/Work/layterz/experiments/venv/lib/python3.10/site-packages/seaborn/utils.py:61: UserWarning:\n",
      "\n",
      "Glyph 141 (\\x8d) missing from current font.\n",
      "\n",
      "/Users/richardlayte/Work/layterz/experiments/venv/lib/python3.10/site-packages/seaborn/utils.py:61: UserWarning:\n",
      "\n",
      "Glyph 146 (\\x92) missing from current font.\n",
      "\n",
      "/Users/richardlayte/Work/layterz/experiments/venv/lib/python3.10/site-packages/seaborn/utils.py:61: UserWarning:\n",
      "\n",
      "Glyph 151 (\\x97) missing from current font.\n",
      "\n",
      "/Users/richardlayte/Work/layterz/experiments/venv/lib/python3.10/site-packages/seaborn/utils.py:61: UserWarning:\n",
      "\n",
      "Glyph 156 (\\x9c) missing from current font.\n",
      "\n",
      "/Users/richardlayte/Work/layterz/experiments/venv/lib/python3.10/site-packages/seaborn/utils.py:61: UserWarning:\n",
      "\n",
      "Glyph 130 (\\x82) missing from current font.\n",
      "\n",
      "/Users/richardlayte/Work/layterz/experiments/venv/lib/python3.10/site-packages/seaborn/utils.py:61: UserWarning:\n",
      "\n",
      "Glyph 135 (\\x87) missing from current font.\n",
      "\n",
      "/Users/richardlayte/Work/layterz/experiments/venv/lib/python3.10/site-packages/seaborn/utils.py:61: UserWarning:\n",
      "\n",
      "Glyph 140 (\\x8c) missing from current font.\n",
      "\n",
      "/Users/richardlayte/Work/layterz/experiments/venv/lib/python3.10/site-packages/seaborn/utils.py:61: UserWarning:\n",
      "\n",
      "Glyph 145 (\\x91) missing from current font.\n",
      "\n",
      "/Users/richardlayte/Work/layterz/experiments/venv/lib/python3.10/site-packages/seaborn/utils.py:61: UserWarning:\n",
      "\n",
      "Glyph 150 (\\x96) missing from current font.\n",
      "\n",
      "/Users/richardlayte/Work/layterz/experiments/venv/lib/python3.10/site-packages/seaborn/utils.py:61: UserWarning:\n",
      "\n",
      "Glyph 155 (\\x9b) missing from current font.\n",
      "\n",
      "/Users/richardlayte/Work/layterz/experiments/venv/lib/python3.10/site-packages/seaborn/utils.py:61: UserWarning:\n",
      "\n",
      "Glyph 131 (\\x83) missing from current font.\n",
      "\n",
      "/Users/richardlayte/Work/layterz/experiments/venv/lib/python3.10/site-packages/seaborn/utils.py:61: UserWarning:\n",
      "\n",
      "Glyph 136 (\\x88) missing from current font.\n",
      "\n",
      "/Users/richardlayte/Work/layterz/experiments/venv/lib/python3.10/site-packages/seaborn/utils.py:61: UserWarning:\n",
      "\n",
      "Glyph 141 (\\x8d) missing from current font.\n",
      "\n",
      "/Users/richardlayte/Work/layterz/experiments/venv/lib/python3.10/site-packages/seaborn/utils.py:61: UserWarning:\n",
      "\n",
      "Glyph 146 (\\x92) missing from current font.\n",
      "\n",
      "/Users/richardlayte/Work/layterz/experiments/venv/lib/python3.10/site-packages/seaborn/utils.py:61: UserWarning:\n",
      "\n",
      "Glyph 151 (\\x97) missing from current font.\n",
      "\n",
      "/Users/richardlayte/Work/layterz/experiments/venv/lib/python3.10/site-packages/seaborn/utils.py:61: UserWarning:\n",
      "\n",
      "Glyph 156 (\\x9c) missing from current font.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76b22b44efd34a0a90d147d6a4d19b10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value=\"<h2 style='font-size: 14; text-align: center;'>Shared tokens</h2>\"), HTML(value=\"<p…"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = shared_tokens(df, 'q')\n",
    "k = shared_tokens(df, 'k')\n",
    "v = shared_tokens(df, 'v')\n",
    "\n",
    "fig, axs = plt.subplots(2, 2, figsize=(12, 10))\n",
    "visualize_shared_tokens(hp, ax=axs[0][0])\n",
    "visualize_shared_tokens(q, ax=axs[0][1])\n",
    "visualize_shared_tokens(k, ax=axs[1][0])\n",
    "visualize_shared_tokens(v, ax=axs[1][1])\n",
    "\n",
    "figure(\n",
    "    fig,\n",
    "    title=\"Shared tokens\",\n",
    "    description=\"\"\"\n",
    "    Heatmap of how tokens are shared between pairs of heads for element-wise, query, key, and value tokens from top clockwise.\n",
    "    \"\"\",\n",
    "    footer=\"IMDB dataset\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random inputs\n",
    "\n",
    "Testing with random token inputs provides some insights into how the heads behave in a \"default\" state. It also makes it easy to change the input length without worrying about being distracted by how that affects the semantic/grammatical structure of the input.\n",
    "\n",
    "Here are the heads of the first layer using a single random token repeated 62 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10eb05611a234fb487e8e9a75631b671",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value=\"<h2 style='font-size: 14; text-align: center;'>Random single repeating token</h2>\")…"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token = random.randint(0, 50257)\n",
    "prompt = torch.full((1, 31), token)\n",
    "cache = run_prompts(model, *model.to_string(prompt))\n",
    "\n",
    "plot, axs = plt.subplots(3, 4, figsize=(16, 12))\n",
    "plt.subplots_adjust(wspace=0, hspace=0.5)\n",
    "for i in range(12):\n",
    "    attn_data = calculate_attns(cache, 0, i)\n",
    "    plot_attn(cache, attn_data, ax=axs[i // 4, i % 4], hide_labels=True, title=f\"Head {i}\")\n",
    "\n",
    "figure(plot, title=\"Random single repeating token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60f468174960452aafea455cf43ca431",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value=\"<h2 style='font-size: 14; text-align: center;'>Random tokens</h2>\"), Output()), lay…"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = torch.randint(0, 50257, (1, 31))\n",
    "cache = run_prompts(model, *model.to_string(prompt))\n",
    "\n",
    "plot, axs = plt.subplots(3, 4, figsize=(16, 12))\n",
    "plt.subplots_adjust(wspace=0, hspace=0.5)\n",
    "for i in range(12):\n",
    "    attn_data = calculate_attns(cache, 0, i)\n",
    "    plot_attn(cache, attn_data, ax=axs[i // 4, i % 4], hide_labels=True, title=f\"Head {i}\")\n",
    "\n",
    "figure(plot, title=\"Random tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24f4022611c54b5cae71baab67e1f8e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value=\"<h2 style='font-size: 14; text-align: center;'>Random repeating 3-seq token</h2>\"),…"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = torch.randint(0, 50257, (1, 3)).repeat(1, 10)\n",
    "cache = run_prompts(model, *model.to_string(prompt))\n",
    "\n",
    "plot, axs = plt.subplots(3, 4, figsize=(16, 12))\n",
    "plt.subplots_adjust(wspace=0, hspace=0.5)\n",
    "for i in range(12):\n",
    "    attn_data = calculate_attns(cache, 0, i)\n",
    "    plot_attn(cache, attn_data, ax=axs[i // 4, i % 4], hide_labels=True, title=f\"Head {i}\")\n",
    "\n",
    "figure(plot, title=\"Random repeating 3-seq token\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the specific patterns change, if you run this multiple times you can see clear motifs emerge that identify specific heads. Again, to get a better idea of how the attention tokens a distributed across heads we can look at the t-SNE plots and shared tokens. The t-SNE plots show similar patterns for the element-wise tokens, but much simpler more structured outputs for q, k and v tokens. This makes sense given the constrained input/output of the random sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Head</th>\n",
       "      <th>Token</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>Token str</th>\n",
       "      <th>Rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>62</td>\n",
       "      <td>,</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>262</td>\n",
       "      <td>31</td>\n",
       "      <td>the</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>287</td>\n",
       "      <td>16743</td>\n",
       "      <td>in</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>31</td>\n",
       "      <td>,</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>262</td>\n",
       "      <td>15701</td>\n",
       "      <td>the</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1252</th>\n",
       "      <td>152</td>\n",
       "      <td>41744</td>\n",
       "      <td>10</td>\n",
       "      <td>antid</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1253</th>\n",
       "      <td>152</td>\n",
       "      <td>43617</td>\n",
       "      <td>15456</td>\n",
       "      <td>Whit</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1254</th>\n",
       "      <td>152</td>\n",
       "      <td>46800</td>\n",
       "      <td>855</td>\n",
       "      <td>TMZ</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1255</th>\n",
       "      <td>153</td>\n",
       "      <td>16100</td>\n",
       "      <td>4</td>\n",
       "      <td>retty</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1256</th>\n",
       "      <td>153</td>\n",
       "      <td>43617</td>\n",
       "      <td>2464</td>\n",
       "      <td>Whit</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1257 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Head  Token  Frequency Token str  Rank\n",
       "0        0     11         62         ,   2.0\n",
       "1        0    262         31       the   3.0\n",
       "2        0    287      16743        in   1.0\n",
       "3        1     11         31         ,   3.0\n",
       "4        1    262      15701       the   1.0\n",
       "...    ...    ...        ...       ...   ...\n",
       "1252   152  41744         10     antid   5.0\n",
       "1253   152  43617      15456      Whit   1.0\n",
       "1254   152  46800        855       TMZ   2.0\n",
       "1255   153  16100          4     retty   2.0\n",
       "1256   153  43617       2464      Whit   1.0\n",
       "\n",
       "[1257 rows x 5 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = load('../data/32x32-random_attn.csv')\n",
    "token_counts = token_freq_data(model, df, 4, (144, 32, 32, 32))\n",
    "q_token_counts = token_freq_data(model, df, 5, (144, 32, 32, 32))\n",
    "k_token_counts = token_freq_data(model, df, 6, (144, 32, 32, 32))\n",
    "v_token_counts = token_freq_data(model, df, 7, (144, 32, 32, 32))\n",
    "v_token_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12988d38f54c46448c70808b04c874cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value=\"<h2 style='font-size: 14; text-align: center;'>t-SNE for attention token embeddings…"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(2, 2, figsize=(12, 8))\n",
    "plot_token_embeddings(model, token_counts, 'TSNE', ax=axs[0][0])\n",
    "plot_token_embeddings(model, q_token_counts, 'TSNE', ax=axs[0][1])\n",
    "plot_token_embeddings(model, k_token_counts, 'TSNE', ax=axs[1][0])\n",
    "plot_token_embeddings(model, v_token_counts, 'TSNE', ax=axs[1][1])\n",
    "\n",
    "figure(\n",
    "    fig,\n",
    "    title=\"t-SNE for attention token embeddings\",\n",
    "    description=\"\"\"\n",
    "    t-SNE representation of the embeddings of the element-wise, query, key, and value attention tokens from top clockwise.\n",
    "    Colours represent the layer index.\n",
    "    \"\"\",\n",
    "    footer=\"Random sequence dataset\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shared token plots are similar to the natural language examples, but there's less structure and a slight hyperbolic nature to some regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/richardlayte/Work/layterz/experiments/venv/lib/python3.10/site-packages/seaborn/utils.py:61: UserWarning:\n",
      "\n",
      "Glyph 130 (\\x82) missing from current font.\n",
      "\n",
      "/Users/richardlayte/Work/layterz/experiments/venv/lib/python3.10/site-packages/seaborn/utils.py:61: UserWarning:\n",
      "\n",
      "Glyph 135 (\\x87) missing from current font.\n",
      "\n",
      "/Users/richardlayte/Work/layterz/experiments/venv/lib/python3.10/site-packages/seaborn/utils.py:61: UserWarning:\n",
      "\n",
      "Glyph 141 (\\x8d) missing from current font.\n",
      "\n",
      "/Users/richardlayte/Work/layterz/experiments/venv/lib/python3.10/site-packages/seaborn/utils.py:61: UserWarning:\n",
      "\n",
      "Glyph 145 (\\x91) missing from current font.\n",
      "\n",
      "/Users/richardlayte/Work/layterz/experiments/venv/lib/python3.10/site-packages/seaborn/utils.py:61: UserWarning:\n",
      "\n",
      "Glyph 153 (\\x99) missing from current font.\n",
      "\n",
      "/Users/richardlayte/Work/layterz/experiments/venv/lib/python3.10/site-packages/seaborn/utils.py:61: UserWarning:\n",
      "\n",
      "Glyph 159 (\\x9f) missing from current font.\n",
      "\n",
      "/Users/richardlayte/Work/layterz/experiments/venv/lib/python3.10/site-packages/seaborn/utils.py:61: UserWarning:\n",
      "\n",
      "Glyph 129 (\\x81) missing from current font.\n",
      "\n",
      "/Users/richardlayte/Work/layterz/experiments/venv/lib/python3.10/site-packages/seaborn/utils.py:61: UserWarning:\n",
      "\n",
      "Glyph 133 (\\x85) missing from current font.\n",
      "\n",
      "/Users/richardlayte/Work/layterz/experiments/venv/lib/python3.10/site-packages/seaborn/utils.py:61: UserWarning:\n",
      "\n",
      "Glyph 139 (\\x8b) missing from current font.\n",
      "\n",
      "/Users/richardlayte/Work/layterz/experiments/venv/lib/python3.10/site-packages/seaborn/utils.py:61: UserWarning:\n",
      "\n",
      "Glyph 144 (\\x90) missing from current font.\n",
      "\n",
      "/Users/richardlayte/Work/layterz/experiments/venv/lib/python3.10/site-packages/seaborn/utils.py:61: UserWarning:\n",
      "\n",
      "Glyph 149 (\\x95) missing from current font.\n",
      "\n",
      "/Users/richardlayte/Work/layterz/experiments/venv/lib/python3.10/site-packages/seaborn/utils.py:61: UserWarning:\n",
      "\n",
      "Glyph 157 (\\x9d) missing from current font.\n",
      "\n",
      "/Users/richardlayte/Work/layterz/experiments/venv/lib/python3.10/site-packages/seaborn/utils.py:61: UserWarning:\n",
      "\n",
      "Glyph 140 (\\x8c) missing from current font.\n",
      "\n",
      "/Users/richardlayte/Work/layterz/experiments/venv/lib/python3.10/site-packages/seaborn/utils.py:61: UserWarning:\n",
      "\n",
      "Glyph 150 (\\x96) missing from current font.\n",
      "\n",
      "/Users/richardlayte/Work/layterz/experiments/venv/lib/python3.10/site-packages/seaborn/utils.py:61: UserWarning:\n",
      "\n",
      "Glyph 156 (\\x9c) missing from current font.\n",
      "\n",
      "/Users/richardlayte/Work/layterz/experiments/venv/lib/python3.10/site-packages/seaborn/utils.py:61: UserWarning:\n",
      "\n",
      "Glyph 130 (\\x82) missing from current font.\n",
      "\n",
      "/Users/richardlayte/Work/layterz/experiments/venv/lib/python3.10/site-packages/seaborn/utils.py:61: UserWarning:\n",
      "\n",
      "Glyph 135 (\\x87) missing from current font.\n",
      "\n",
      "/Users/richardlayte/Work/layterz/experiments/venv/lib/python3.10/site-packages/seaborn/utils.py:61: UserWarning:\n",
      "\n",
      "Glyph 145 (\\x91) missing from current font.\n",
      "\n",
      "/Users/richardlayte/Work/layterz/experiments/venv/lib/python3.10/site-packages/seaborn/utils.py:61: UserWarning:\n",
      "\n",
      "Glyph 131 (\\x83) missing from current font.\n",
      "\n",
      "/Users/richardlayte/Work/layterz/experiments/venv/lib/python3.10/site-packages/seaborn/utils.py:61: UserWarning:\n",
      "\n",
      "Glyph 136 (\\x88) missing from current font.\n",
      "\n",
      "/Users/richardlayte/Work/layterz/experiments/venv/lib/python3.10/site-packages/seaborn/utils.py:61: UserWarning:\n",
      "\n",
      "Glyph 146 (\\x92) missing from current font.\n",
      "\n",
      "/Users/richardlayte/Work/layterz/experiments/venv/lib/python3.10/site-packages/seaborn/utils.py:61: UserWarning:\n",
      "\n",
      "Glyph 151 (\\x97) missing from current font.\n",
      "\n",
      "/Users/richardlayte/Work/layterz/experiments/venv/lib/python3.10/site-packages/seaborn/utils.py:61: UserWarning:\n",
      "\n",
      "Glyph 141 (\\x8d) missing from current font.\n",
      "\n",
      "/Users/richardlayte/Work/layterz/experiments/venv/lib/python3.10/site-packages/seaborn/utils.py:61: UserWarning:\n",
      "\n",
      "Glyph 137 (\\x89) missing from current font.\n",
      "\n",
      "/Users/richardlayte/Work/layterz/experiments/venv/lib/python3.10/site-packages/seaborn/utils.py:61: UserWarning:\n",
      "\n",
      "Glyph 142 (\\x8e) missing from current font.\n",
      "\n",
      "/Users/richardlayte/Work/layterz/experiments/venv/lib/python3.10/site-packages/seaborn/utils.py:61: UserWarning:\n",
      "\n",
      "Glyph 154 (\\x9a) missing from current font.\n",
      "\n",
      "/Users/richardlayte/Work/layterz/experiments/venv/lib/python3.10/site-packages/seaborn/utils.py:61: UserWarning:\n",
      "\n",
      "Glyph 129 (\\x81) missing from current font.\n",
      "\n",
      "/Users/richardlayte/Work/layterz/experiments/venv/lib/python3.10/site-packages/seaborn/utils.py:61: UserWarning:\n",
      "\n",
      "Glyph 133 (\\x85) missing from current font.\n",
      "\n",
      "/Users/richardlayte/Work/layterz/experiments/venv/lib/python3.10/site-packages/seaborn/utils.py:61: UserWarning:\n",
      "\n",
      "Glyph 149 (\\x95) missing from current font.\n",
      "\n",
      "/Users/richardlayte/Work/layterz/experiments/venv/lib/python3.10/site-packages/seaborn/utils.py:61: UserWarning:\n",
      "\n",
      "Glyph 134 (\\x86) missing from current font.\n",
      "\n",
      "/Users/richardlayte/Work/layterz/experiments/venv/lib/python3.10/site-packages/seaborn/utils.py:61: UserWarning:\n",
      "\n",
      "Glyph 138 (\\x8a) missing from current font.\n",
      "\n",
      "/Users/richardlayte/Work/layterz/experiments/venv/lib/python3.10/site-packages/seaborn/utils.py:61: UserWarning:\n",
      "\n",
      "Glyph 143 (\\x8f) missing from current font.\n",
      "\n",
      "/Users/richardlayte/Work/layterz/experiments/venv/lib/python3.10/site-packages/seaborn/utils.py:61: UserWarning:\n",
      "\n",
      "Glyph 148 (\\x94) missing from current font.\n",
      "\n",
      "/Users/richardlayte/Work/layterz/experiments/venv/lib/python3.10/site-packages/seaborn/utils.py:61: UserWarning:\n",
      "\n",
      "Glyph 130 (\\x82) missing from current font.\n",
      "\n",
      "/Users/richardlayte/Work/layterz/experiments/venv/lib/python3.10/site-packages/seaborn/utils.py:61: UserWarning:\n",
      "\n",
      "Glyph 153 (\\x99) missing from current font.\n",
      "\n",
      "/Users/richardlayte/Work/layterz/experiments/venv/lib/python3.10/site-packages/seaborn/utils.py:61: UserWarning:\n",
      "\n",
      "Glyph 159 (\\x9f) missing from current font.\n",
      "\n",
      "/Users/richardlayte/Work/layterz/experiments/venv/lib/python3.10/site-packages/seaborn/utils.py:61: UserWarning:\n",
      "\n",
      "Glyph 127 () missing from current font.\n",
      "\n",
      "/Users/richardlayte/Work/layterz/experiments/venv/lib/python3.10/site-packages/seaborn/utils.py:61: UserWarning:\n",
      "\n",
      "Glyph 152 (\\x98) missing from current font.\n",
      "\n",
      "/Users/richardlayte/Work/layterz/experiments/venv/lib/python3.10/site-packages/seaborn/utils.py:61: UserWarning:\n",
      "\n",
      "Glyph 131 (\\x83) missing from current font.\n",
      "\n",
      "/Users/richardlayte/Work/layterz/experiments/venv/lib/python3.10/site-packages/seaborn/utils.py:61: UserWarning:\n",
      "\n",
      "Glyph 135 (\\x87) missing from current font.\n",
      "\n",
      "/Users/richardlayte/Work/layterz/experiments/venv/lib/python3.10/site-packages/seaborn/utils.py:61: UserWarning:\n",
      "\n",
      "Glyph 139 (\\x8b) missing from current font.\n",
      "\n",
      "/Users/richardlayte/Work/layterz/experiments/venv/lib/python3.10/site-packages/seaborn/utils.py:61: UserWarning:\n",
      "\n",
      "Glyph 143 (\\x8f) missing from current font.\n",
      "\n",
      "/Users/richardlayte/Work/layterz/experiments/venv/lib/python3.10/site-packages/seaborn/utils.py:61: UserWarning:\n",
      "\n",
      "Glyph 148 (\\x94) missing from current font.\n",
      "\n",
      "/Users/richardlayte/Work/layterz/experiments/venv/lib/python3.10/site-packages/seaborn/utils.py:61: UserWarning:\n",
      "\n",
      "Glyph 157 (\\x9d) missing from current font.\n",
      "\n",
      "/Users/richardlayte/Work/layterz/experiments/venv/lib/python3.10/site-packages/seaborn/utils.py:61: UserWarning:\n",
      "\n",
      "Glyph 132 (\\x84) missing from current font.\n",
      "\n",
      "/Users/richardlayte/Work/layterz/experiments/venv/lib/python3.10/site-packages/seaborn/utils.py:61: UserWarning:\n",
      "\n",
      "Glyph 138 (\\x8a) missing from current font.\n",
      "\n",
      "/Users/richardlayte/Work/layterz/experiments/venv/lib/python3.10/site-packages/seaborn/utils.py:61: UserWarning:\n",
      "\n",
      "Glyph 153 (\\x99) missing from current font.\n",
      "\n",
      "/Users/richardlayte/Work/layterz/experiments/venv/lib/python3.10/site-packages/seaborn/utils.py:61: UserWarning:\n",
      "\n",
      "Glyph 159 (\\x9f) missing from current font.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "183148579249406da0cf676cc8879ec7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value=\"<h2 style='font-size: 14; text-align: center;'>Shared tokens</h2>\"), HTML(value=\"<p…"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hp = shared_tokens(df)\n",
    "q = shared_tokens(df, 'q')\n",
    "k = shared_tokens(df, 'k')\n",
    "v = shared_tokens(df, 'v')\n",
    "\n",
    "fig, axs = plt.subplots(2, 2, figsize=(12, 10))\n",
    "visualize_shared_tokens(hp, ax=axs[0][0])\n",
    "visualize_shared_tokens(q, ax=axs[0][1])\n",
    "visualize_shared_tokens(k, ax=axs[1][0])\n",
    "visualize_shared_tokens(v, ax=axs[1][1])\n",
    "\n",
    "figure(\n",
    "    fig,\n",
    "    title=\"Shared tokens\",\n",
    "    description=\"\"\"\n",
    "    Heatmap of how tokens are shared between pairs of heads for element-wise, query, key, and value tokens from top clockwise.\n",
    "    \"\"\",\n",
    "    footer=\"Random sequence dataset\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specific head analysis\n",
    "\n",
    "Random repeating sequences provides the clearest interpretability without being devoid of structure so I'm starting there. Understanding how heads evolve based on changing predictable sequences is likely to provide more general insights, but it's worth noting again the risk of these toy inputs not translating to more realistic language.\n",
    "\n",
    "To provide some structure to the work I plan to evaluate the patterns using this approach:\n",
    "- Take an input sequence (S) and convert it into a sequence of attention tokens (A)\n",
    "- Given the next token (t) generate a new sequence of attention tokens using S + t\n",
    "- Analyse the invariances and symmetries implied by how different sequences produce different attention tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
