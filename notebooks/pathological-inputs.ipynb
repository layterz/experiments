{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pathological inputs\n",
    "\n",
    "Testing attention patterns with various pathelogical inputs (e.g. random, repeated tokens, etc) to try and explore any interesting patterns generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformer_lens import HookedTransformer\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "# NBVAL_IGNORE_OUTPUT\n",
    "model = HookedTransformer.from_pretrained(\n",
    "    \"gpt2-small\",\n",
    "    center_unembed=True,\n",
    "    center_writing_weights=True,\n",
    "    fold_ln=True,\n",
    "    refactor_factored_attn_matrices=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import permutations, product\n",
    "import einops\n",
    "from fancy_einsum import einsum\n",
    "from IPython.display import HTML\n",
    "from circuitsvis.attention import attention_heads\n",
    "from plotly import express as px\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = \"png\"\n",
    "\n",
    "def calculate_logit_diff(cache):\n",
    "    pred_tokens = torch.tensor([\n",
    "        [model.to_single_token(c) for c in completions] \n",
    "        for _, completions in cache.prompts\n",
    "    ]).to(device)\n",
    "\n",
    "    resid_directions = cache.model.tokens_to_residual_directions(pred_tokens)\n",
    "    return resid_directions[:, 0] - resid_directions[:, 1]\n",
    "\n",
    "def calculate_head_contribution(cache, towards, layer=-1, pos_slice=-1):\n",
    "    per_head_residual = cache.stack_head_results(\n",
    "        layer=layer, pos_slice=pos_slice,\n",
    "    )\n",
    "\n",
    "    per_head_logit_diffs = einsum(\n",
    "        \"... batch d_model, batch d_model -> ...\",\n",
    "        per_head_residual, towards,\n",
    "    )\n",
    "\n",
    "    return einops.rearrange(\n",
    "        per_head_logit_diffs,\n",
    "        \"(layer head_index) -> layer head_index\",\n",
    "        layer=cache.model.cfg.n_layers,\n",
    "        head_index=cache.model.cfg.n_heads,\n",
    "    )\n",
    "\n",
    "def visualize_attention_patterns(heads, cache, token_labels=None):\n",
    "    labels, patterns = [], []\n",
    "    for head in heads:\n",
    "        layer = head // cache.model.cfg.n_heads\n",
    "        head_index = head % cache.model.cfg.n_heads\n",
    "        labels.append(f\"L{layer}H{head_index}\")\n",
    "        patterns.append(cache[\"attn\", layer][0, head_index])\n",
    "\n",
    "    patterns = torch.stack(patterns, dim=0)\n",
    "    if token_labels is None:\n",
    "        token_labels = [f\"Token {i}\" for i in range(cache['attn', 0].shape[-1])]\n",
    "\n",
    "    return attention_heads(\n",
    "        attention=patterns, tokens=token_labels, attention_head_names=labels\n",
    "    ).show_code()\n",
    "\n",
    "def generate_prompts(t):\n",
    "    names = (\" Mary\", \" John\", \" Alice\")\n",
    "    places = (\" shops\", \" park\", \" beach\")\n",
    "    things = (\" bag\", \" ball\", \" book\")\n",
    "\n",
    "    templates = [\n",
    "        (\n",
    "            t, (place, thing), permutations(names, 2),\n",
    "        )\n",
    "        for place, thing in product(places, things)\n",
    "    ]\n",
    "\n",
    "    return [\n",
    "        (prompt.format(place, thing, S, IO), (IO, S))\n",
    "        for prompt, (place, thing), names in templates\n",
    "        for S, IO in names\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random tokens\n",
    "\n",
    "Input is generated by selecting 15 random tokens in a batch of 100 prompts. The attention head contributions are then plotted based on their contribution to a random token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = torch.randint(0, 50257, (100, 15)).to(device)\n",
    "_, cache = model.run_with_cache(tokens)\n",
    "random_token = torch.randint(0, 50257, (1, 1)).to(device)\n",
    "random_token_direction = cache.model.tokens_to_residual_directions(random_token)\n",
    "heads = calculate_head_contribution(cache, random_token_direction.unsqueeze(0))\n",
    "\n",
    "px.imshow(\n",
    "    heads.cpu(),\n",
    "    labels={\"x\": \"Head\", \"y\": \"Layer\"},\n",
    "    title=\"Logit Difference From Each Head\",\n",
    "    color_continuous_midpoint=0.0,\n",
    "    color_continuous_scale=\"RdBu\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the attention patterns is interesting because some heads tend towards random noise (e.g. 0.0, 0.2, 0.6, etc) while others are in some kind of \"default\" state (e.g. 0.1, 0.3, 0.4, etc).\n",
    "\n",
    "Perhaps this points to heads which focus, broadly, on token correlations and semantic structure respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(visualize_attention_patterns(range(144), cache))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random repeated token\n",
    "\n",
    "Input is a batch of 100 random tokens, each repeated 15 times. The head contributions are plotted against the direction of the same random token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_token = torch.randint(0, 50257, (100, 1)).to(device)\n",
    "random_tokens = random_token.expand(-1, 15)\n",
    "random_token_direction = cache.model.tokens_to_residual_directions(random_token)\n",
    "_, cache = model.run_with_cache(random_tokens)\n",
    "heads = calculate_head_contribution(cache, random_token_direction.squeeze(1))\n",
    "\n",
    "px.imshow(\n",
    "    heads.cpu(),\n",
    "    labels={\"x\": \"Head\", \"y\": \"Layer\"},\n",
    "    title=\"Logit Difference From Each Head\",\n",
    "    color_continuous_midpoint=0.0,\n",
    "    color_continuous_scale=\"RdBu\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The attention patterns are what we'd expect to see if we make the assumption that heads learn via in-context learning - i.e. there are no patterns to capture other than the initial repeating token so we see smooth gradients with a few exceptions that quickly converge. \n",
    "\n",
    "4.11 is a notable exception."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(visualize_attention_patterns(range(144), cache))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repeated random subsequence\n",
    "\n",
    "Input is generated as a random 3 token sequence which is then repeated 5 times. Head attribution is plotted against the direction of the first token in the subsequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_tokens = torch.randint(0, 50257, (100, 3)).to(device)\n",
    "random_tokens = random_tokens.repeat(1, 5)\n",
    "random_token = torch.randint(0, 50257, (1, 1)).to(device)\n",
    "first_tokens = torch.index_select(random_tokens, 1, torch.tensor([0]).to(device)).to(device)\n",
    "first_token_direction = cache.model.tokens_to_residual_directions(first_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_tokens = torch.randint(0, 50257, (100, 3)).to(device)\n",
    "random_tokens = random_tokens.repeat(1, 5)\n",
    "first_tokens = torch.index_select(random_tokens, 1, torch.tensor([0]).to(device)).to(device)\n",
    "first_token_direction = cache.model.tokens_to_residual_directions(first_tokens)\n",
    "_, cache = model.run_with_cache(random_tokens)\n",
    "heads = calculate_head_contribution(cache, first_token_direction.squeeze(1))\n",
    "\n",
    "fig = px.imshow(\n",
    "    heads.cpu(),\n",
    "    labels={\"x\": \"Head\", \"y\": \"Layer\"},\n",
    "    title=\"Logit Difference From Each Head\",\n",
    "    color_continuous_midpoint=0.0,\n",
    "    color_continuous_scale=\"RdBu\",\n",
    ")\n",
    "\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The attention patterns follow a similar pattern to the repeated token experiment in that smooth gradients represent repeating tokens, but this time for each token separately. This leads to some interesting properties that start to emerge as they intersect.\n",
    "\n",
    "For example, the first head (0.0) attends to the input sequence depending on where the current token is in the subsequence. When the current token is A in the subsequence A, B, C it attends mostly to all previous C tokens, with scores ordered by most recent token. E.g. token 9 attends to 8, 5, 2 in that order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(visualize_attention_patterns(range(144), cache))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
